"""Generate structured scientific papers from completed investigations.

Assembles investigation data + persisted events into a publication-format paper.
Pure function -- no I/O, no external dependencies.
"""

from __future__ import annotations

import json
from typing import Any


def generate_paper(
    investigation_id: str,
    prompt: str,
    summary: str,
    domain: str,
    created_at: str,
    hypotheses: list[dict[str, Any]],
    experiments: list[dict[str, Any]],
    findings: list[dict[str, Any]],
    candidates: list[dict[str, Any]],
    negative_controls: list[dict[str, Any]],
    positive_controls: list[dict[str, Any]],
    citations: list[str],
    cost_data: dict[str, Any],
    events: list[dict[str, str]],
) -> dict[str, str]:
    """Generate a structured scientific paper as a dict of Markdown sections.

    Each key is a section name, each value is Markdown content.
    The ``full_markdown`` key contains all sections combined.
    """
    parsed_events = _parse_events(events)
    pico = _extract_pico(parsed_events)
    lit_survey = _extract_literature_survey(parsed_events)
    evaluations = _extract_evaluations(parsed_events)
    validation = _extract_validation_metrics(parsed_events)
    tool_usage = _extract_tool_usage(parsed_events)

    sections: dict[str, str] = {}

    sections["title"] = _build_title(prompt, domain, created_at, investigation_id)
    sections["abstract"] = _build_abstract(summary)
    sections["introduction"] = _build_introduction(prompt, pico, lit_survey)
    sections["methods"] = _build_methods(hypotheses, experiments, tool_usage)
    sections["results"] = _build_results(hypotheses, findings, candidates)
    sections["discussion"] = _build_discussion(hypotheses, evaluations)
    sections["references"] = _build_references(citations, findings)
    sections["supplementary"] = _build_supplementary(
        cost_data,
        negative_controls,
        positive_controls,
        validation,
    )

    sections["full_markdown"] = _combine_sections(sections)
    return sections


# ── Event Parsing ──────────────────────────────────────────────────────


def _parse_events(events: list[dict[str, str]]) -> list[dict[str, Any]]:
    """Parse event_data JSON strings into dicts with event_type preserved."""
    parsed: list[dict[str, Any]] = []
    for ev in events:
        try:
            data = json.loads(ev["event_data"])
        except (json.JSONDecodeError, KeyError):
            continue
        # SSE events are wrapped: {"event": "...", "data": {...}}
        inner = data.get("data", data) if isinstance(data, dict) else data
        parsed.append({"event_type": ev.get("event_type", ""), "data": inner})
    return parsed


def _extract_pico(events: list[dict[str, Any]]) -> dict[str, str]:
    for ev in events:
        if ev["event_type"] == "literature_survey_completed":
            pico: dict[str, str] = ev["data"].get("pico", {})
            return pico
    return {}


def _extract_literature_survey(events: list[dict[str, Any]]) -> dict[str, Any]:
    for ev in events:
        if ev["event_type"] == "literature_survey_completed":
            data: dict[str, Any] = ev["data"]
            return data
    return {}


def _extract_evaluations(events: list[dict[str, Any]]) -> dict[str, dict[str, Any]]:
    """Map hypothesis_id -> evaluation data (reasoning, certainty)."""
    evals: dict[str, dict[str, Any]] = {}
    for ev in events:
        if ev["event_type"] == "hypothesis_evaluated":
            hid = ev["data"].get("hypothesis_id", "")
            if hid:
                evals[hid] = ev["data"]
    return evals


def _extract_validation_metrics(events: list[dict[str, Any]]) -> dict[str, Any]:
    for ev in events:
        if ev["event_type"] == "validation_metrics":
            metrics: dict[str, Any] = ev["data"]
            return metrics
    return {}


def _extract_tool_usage(events: list[dict[str, Any]]) -> dict[str, int]:
    """Count tool calls by tool name."""
    counts: dict[str, int] = {}
    for ev in events:
        if ev["event_type"] == "tool_called":
            name = ev["data"].get("tool_name", "unknown")
            counts[name] = counts.get(name, 0) + 1
    return counts


# ── Section Builders ───────────────────────────────────────────────────


def _build_title(
    prompt: str,
    domain: str,
    created_at: str,
    investigation_id: str,
) -> str:
    date = created_at[:10] if created_at else ""
    lines = [
        f"# {prompt}",
        "",
        "_Generated by Ehrlich -- AI Scientific Discovery Engine_  ",
        f"_Domain: {domain or 'auto-detected'} | Date: {date} | ID: {investigation_id}_",
    ]
    return "\n".join(lines)


def _build_abstract(summary: str) -> str:
    if not summary:
        return "## Abstract\n\nNo synthesis available."
    return f"## Abstract\n\n{summary}"


def _build_introduction(
    prompt: str,
    pico: dict[str, str],
    lit_survey: dict[str, Any],
) -> str:
    lines = ["## 1. Introduction"]

    # Research question
    lines.append("")
    lines.append("### 1.1 Research Question")
    lines.append("")
    lines.append(f"> {prompt}")

    # PICO framework
    if pico:
        lines.append("")
        lines.append("### 1.2 PICO Framework")
        lines.append("")
        for key in ("population", "intervention", "comparison", "outcome"):
            val = pico.get(key, "")
            if val:
                lines.append(f"- **{key.title()}:** {val}")
        search_terms = pico.get("search_terms")
        if isinstance(search_terms, list) and search_terms:
            lines.append(f"- **Search Terms:** {', '.join(search_terms)}")

    # Literature survey
    if lit_survey:
        lines.append("")
        lines.append("### 1.3 Literature Survey")
        lines.append("")
        total = lit_survey.get("total_results", 0)
        included = lit_survey.get("included_results", 0)
        grade = lit_survey.get("evidence_grade", "")
        assessment = lit_survey.get("assessment", "")
        lines.append(f"- **Papers reviewed:** {total} found, {included} included")
        if grade:
            lines.append(f"- **Evidence grade:** {grade}")
        if assessment:
            lines.append(f"- **Assessment:** {assessment}")

    return "\n".join(lines)


def _build_methods(
    hypotheses: list[dict[str, Any]],
    experiments: list[dict[str, Any]],
    tool_usage: dict[str, int],
) -> str:
    lines = ["## 2. Methods"]

    if not experiments:
        lines.append("\nNo experiments recorded.")
        return "\n".join(lines)

    # Experimental design per hypothesis
    lines.append("")
    lines.append("### 2.1 Experimental Design")

    for h in hypotheses:
        h_exps = [e for e in experiments if e.get("hypothesis_id") == h["id"]]
        if not h_exps:
            continue

        lines.append("")
        lines.append(f"#### Hypothesis {h['id'][:8]}: {h.get('statement', '')}")
        lines.append("")

        for exp in h_exps:
            lines.append(f"**Experiment {exp.get('id', '')[:8]}:** {exp.get('description', '')}")
            lines.append("")

            iv = exp.get("independent_variable", "")
            dv = exp.get("dependent_variable", "")
            if iv or dv:
                if iv:
                    lines.append(f"- **Independent variable:** {iv}")
                if dv:
                    lines.append(f"- **Dependent variable:** {dv}")

            controls = exp.get("controls") or []
            if controls:
                lines.append(f"- **Controls:** {', '.join(controls)}")

            confounders = exp.get("confounders") or []
            if confounders:
                lines.append(f"- **Confounders:** {', '.join(confounders)}")

            ap = exp.get("analysis_plan", "")
            if ap:
                lines.append(f"- **Analysis plan:** {ap}")

            sc = exp.get("success_criteria", "")
            fc = exp.get("failure_criteria", "")
            if sc:
                lines.append(f"- **Success criteria:** {sc}")
            if fc:
                lines.append(f"- **Failure criteria:** {fc}")

            tool_plan = exp.get("tool_plan") or []
            if tool_plan:
                lines.append(f"- **Tool plan:** {', '.join(tool_plan)}")

            lines.append("")

    # Tool usage summary
    if tool_usage:
        lines.append("### 2.2 Tools & Data Sources")
        lines.append("")
        lines.append("| Tool | Calls |")
        lines.append("|------|-------|")
        for name, count in sorted(tool_usage.items(), key=lambda x: -x[1]):
            lines.append(f"| {name} | {count} |")

    return "\n".join(lines)


def _build_results(
    hypotheses: list[dict[str, Any]],
    findings: list[dict[str, Any]],
    candidates: list[dict[str, Any]],
) -> str:
    lines = ["## 3. Results"]

    # Hypothesis outcomes table
    if hypotheses:
        lines.append("")
        lines.append("### 3.1 Hypothesis Outcomes")
        lines.append("")
        lines.append("| ID | Statement | Status | Confidence | Certainty |")
        lines.append("|----|-----------|--------|------------|-----------|")
        for h in hypotheses:
            hid = h.get("id", "")[:8]
            stmt = h.get("statement", "")[:80]
            status = h.get("status", "")
            conf = h.get("confidence", 0)
            conf_str = f"{conf * 100:.0f}%" if conf > 0 else "-"
            cert = h.get("certainty_of_evidence", "-")
            lines.append(f"| {hid} | {stmt} | {status} | {conf_str} | {cert} |")

    # Key findings by evidence type
    if findings:
        lines.append("")
        lines.append("### 3.2 Key Findings")

        by_type: dict[str, list[dict[str, Any]]] = {
            "supporting": [],
            "contradicting": [],
            "neutral": [],
        }
        for f in findings:
            bucket = by_type.get(f.get("evidence_type", "neutral"), by_type["neutral"])
            bucket.append(f)

        for etype, items in by_type.items():
            if not items:
                continue
            lines.append("")
            lines.append(f"#### {etype.title()} Evidence ({len(items)})")
            lines.append("")
            for f in items:
                source = ""
                st = f.get("source_type", "")
                sid = f.get("source_id", "")
                if st and sid:
                    source = f" [{st}: {sid}]"
                lines.append(f"- **{f.get('title', '')}**{source}")
                detail = f.get("detail", "")
                if detail:
                    lines.append(f"  {detail}")

    # Candidates
    if candidates:
        lines.append("")
        lines.append("### 3.3 Candidates")
        lines.append("")

        score_keys: set[str] = set()
        for c in candidates:
            for k in c.get("scores", {}):
                score_keys.add(k)

        headers = ["Rank", "Name", "Identifier"]
        sorted_keys = sorted(score_keys)
        headers.extend(k.replace("_", " ") for k in sorted_keys)

        lines.append(f"| {' | '.join(headers)} |")
        lines.append(f"|{'|'.join('------' for _ in headers)}|")

        for c in candidates:
            scores = c.get("scores", {})
            row = [
                str(c.get("rank", "")),
                c.get("name", "-"),
                f"`{c.get('identifier', '')}`",
            ]
            for k in sorted_keys:
                v = scores.get(k)
                row.append(f"{v:.2f}" if isinstance(v, (int, float)) else "-")
            lines.append(f"| {' | '.join(row)} |")

    return "\n".join(lines)


def _build_discussion(
    hypotheses: list[dict[str, Any]],
    evaluations: dict[str, dict[str, Any]],
) -> str:
    lines = ["## 4. Discussion"]

    if not hypotheses:
        lines.append("\nNo hypotheses to discuss.")
        return "\n".join(lines)

    lines.append("")
    lines.append("### 4.1 Hypothesis Assessment")

    for h in hypotheses:
        hid = h.get("id", "")
        lines.append("")
        lines.append(f"#### {h.get('statement', '')}")
        lines.append("")

        status = h.get("status", "")
        conf = h.get("confidence", 0)
        cert = h.get("certainty_of_evidence", "")
        lines.append(f"- **Status:** {status}")
        if conf > 0:
            lines.append(f"- **Confidence:** {conf * 100:.0f}%")
        if cert:
            lines.append(f"- **Certainty of evidence:** {cert}")

        # Add evaluation reasoning from events
        ev = evaluations.get(hid, {})
        reasoning = ev.get("reasoning", "")
        if reasoning:
            lines.append(f"- **Reasoning:** {reasoning}")

        # Supporting / contradicting evidence
        sup = h.get("supporting_evidence") or []
        con = h.get("contradicting_evidence") or []
        if sup:
            lines.append(f"- **Supporting evidence:** {'; '.join(sup)}")
        if con:
            lines.append(f"- **Contradicting evidence:** {'; '.join(con)}")

    return "\n".join(lines)


def _build_references(
    citations: list[str],
    findings: list[dict[str, Any]],
) -> str:
    lines = ["## References"]

    if not citations and not findings:
        lines.append("\nNo references collected.")
        return "\n".join(lines)

    # Formal citations
    if citations:
        lines.append("")
        for i, cit in enumerate(citations, 1):
            lines.append(f"{i}. {cit}")

    # Source provenance from findings (unique sources)
    sources: set[str] = set()
    for f in findings:
        st = f.get("source_type", "")
        sid = f.get("source_id", "")
        if st and sid:
            sources.add(f"{st}: {sid}")

    if sources:
        lines.append("")
        lines.append("### Data Sources")
        lines.append("")
        for src in sorted(sources):
            lines.append(f"- {src}")

    return "\n".join(lines)


def _build_supplementary(
    cost_data: dict[str, Any],
    negative_controls: list[dict[str, Any]],
    positive_controls: list[dict[str, Any]],
    validation: dict[str, Any],
) -> str:
    lines = ["## Supplementary Material"]

    # Model validation
    if negative_controls or positive_controls:
        lines.append("")
        lines.append("### S1. Model Validation")

        if negative_controls:
            correct = sum(1 for nc in negative_controls if nc.get("correctly_classified"))
            total = len(negative_controls)
            lines.append("")
            lines.append(f"**Negative controls:** {correct}/{total} correctly classified")
            lines.append("")
            lines.append("| Name | Identifier | Score | Classified |")
            lines.append("|------|------------|-------|------------|")
            for nc in negative_controls:
                name = nc.get("name", "-")
                ident = nc.get("identifier", "")
                score = nc.get("score", 0)
                ok = "Pass" if nc.get("correctly_classified") else "Fail"
                lines.append(f"| {name} | `{ident}` | {score:.3f} | {ok} |")

        if positive_controls:
            correct = sum(1 for pc in positive_controls if pc.get("correctly_classified"))
            total = len(positive_controls)
            lines.append("")
            lines.append(f"**Positive controls:** {correct}/{total} correctly classified")
            lines.append("")
            lines.append("| Name | Identifier | Known Activity | Score | Classified |")
            lines.append("|------|------------|----------------|-------|------------|")
            for pc in positive_controls:
                name = pc.get("name", "-")
                ident = pc.get("identifier", "")
                activity = pc.get("known_activity", "")
                score = pc.get("score", 0)
                ok = "Pass" if pc.get("correctly_classified") else "Fail"
                lines.append(f"| {name} | `{ident}` | {activity} | {score:.3f} | {ok} |")

        if validation:
            zp = validation.get("z_prime")
            if zp is not None:
                quality = validation.get("z_prime_quality", "")
                lines.append("")
                lines.append(f"**Z'-factor:** {zp:.3f} ({quality})")
                pm = validation.get("positive_mean", 0)
                nm = validation.get("negative_mean", 0)
                pc_count = validation.get("positive_control_count", 0)
                nc_count = validation.get("negative_control_count", 0)
                lines.append(f"- Positive controls: mean={pm:.3f}, n={pc_count}")
                lines.append(f"- Negative controls: mean={nm:.3f}, n={nc_count}")

    # Cost & performance
    if cost_data:
        lines.append("")
        lines.append("### S2. Cost & Performance")
        lines.append("")
        lines.append(f"- **Input tokens:** {cost_data.get('input_tokens', 0):,}")
        lines.append(f"- **Output tokens:** {cost_data.get('output_tokens', 0):,}")
        total_cost = cost_data.get("total_cost_usd", 0)
        lines.append(f"- **Total cost:** ${total_cost:.4f}")
        tool_calls = cost_data.get("tool_calls", 0)
        if tool_calls:
            lines.append(f"- **Tool calls:** {tool_calls}")

        by_model = cost_data.get("by_model")
        if isinstance(by_model, dict) and by_model:
            lines.append("")
            lines.append("| Model | Input | Output | Calls | Cost |")
            lines.append("|-------|-------|--------|-------|------|")
            for model, mc in by_model.items():
                if not isinstance(mc, dict):
                    continue
                inp = mc.get("input_tokens", 0)
                out = mc.get("output_tokens", 0)
                calls = mc.get("calls", 0)
                cost = mc.get("cost_usd", 0)
                lines.append(f"| {model} | {inp:,} | {out:,} | {calls} | ${cost:.4f} |")

    return "\n".join(lines)


def extract_visualizations(events: list[dict[str, str]]) -> list[dict[str, Any]]:
    """Extract visualization payloads from persisted events.

    Returns a list of ``{viz_type, title, data, config}`` dicts -- one per
    ``visualization`` event found in the timeline.
    """
    parsed = _parse_events(events)
    vizs: list[dict[str, Any]] = []
    for ev in parsed:
        if ev["event_type"] == "visualization":
            d = ev["data"]
            vizs.append(
                {
                    "viz_type": d.get("viz_type", ""),
                    "title": d.get("title", ""),
                    "data": d.get("data", {}),
                    "config": d.get("config", {}),
                }
            )
    return vizs


def _combine_sections(sections: dict[str, str]) -> str:
    """Combine all sections into a single Markdown document."""
    ordered = [
        "title",
        "abstract",
        "introduction",
        "methods",
        "results",
        "discussion",
        "references",
        "supplementary",
    ]
    parts: list[str] = []
    for key in ordered:
        content = sections.get(key, "")
        if content:
            parts.append(content)
    return "\n\n---\n\n".join(parts)
