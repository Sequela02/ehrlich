# Synthesis Phase Research

Phase 6 of the [Scientific Methodology Upgrade](../docs/scientific-methodology.md).

## Sources

| # | Author/Framework | Year | Key Contribution | Verified |
|---|-----------------|------|------------------|----------|
| 1 | Guyatt, G.H. et al. (GRADE Working Group) | 2008 | GRADE: emerging consensus on rating quality of evidence and strength of recommendations. *BMJ*, 336(7650), 924-926. DOI: 10.1136/bmj.39489.470347.AD | Yes |
| 2 | Guyatt, G.H. et al. | 2011 | GRADE guidelines 3: Rating the quality of evidence. *J Clin Epidemiol*, 64(4), 401-406. DOI: 10.1016/j.jclinepi.2010.07.015 | Yes |
| 3 | Higgins, J.P.T. et al. | 2019 | Cochrane Handbook for Systematic Reviews of Interventions (v6.4, updated 2023). Wiley. DOI: 10.1002/9781119536604 | Yes |
| 4 | Page, M.J. et al. | 2021 | PRISMA 2020: updated guideline for reporting systematic reviews. *BMJ*, 372, n71. DOI: 10.1136/bmj.n71 | Yes |
| 5 | Popay, J. et al. | 2006 | Guidance on the conduct of narrative synthesis in systematic reviews. ESRC Methods Programme. Lancaster University. | Yes |
| 6 | Campbell, M. et al. | 2020 | Synthesis without meta-analysis (SWiM) reporting guideline. *BMJ*, 368, l6890. DOI: 10.1136/bmj.l6890 | Yes |
| 7 | Derringer, G. & Suich, R. | 1980 | Simultaneous optimization of several response variables. *J Quality Technology*, 12(4), 214-219. DOI: 10.1080/00224065.1980.11980968 | Yes |
| 8 | Harrington, E.C. | 1965 | The desirability function. *Industrial Quality Control*, 21, 494-498. | Yes |
| 9 | Hwang, C.L. & Yoon, K. | 1981 | *Multiple Attribute Decision Making: Methods and Applications*. Springer-Verlag, New York. | Yes |
| 10 | Bickerton, G.R. et al. | 2012 | Quantifying the chemical beauty of drugs (QED). *Nature Chemistry*, 4, 90-98. DOI: 10.1038/nchem.1243 | Yes |
| 11 | Wager, T.T. et al. | 2016 | CNS Multiparameter Optimization Desirability. *ACS Chem Neurosci*, 7(6), 767-775. DOI: 10.1021/acschemneuro.6b00029 | Yes |
| 12 | Nicolaou, C.A. & Brown, N. | 2013 | Multi-objective optimization methods in drug design. *Drug Discov Today: Technologies*, 10(3), e427-435. DOI: 10.1016/j.ddtec.2013.02.001 | Yes |
| 13 | Lewis, S. & Clarke, M. | 2001 | Forest plots: trying to see the wood and the trees. *BMJ*, 322, 1479-1480. DOI: 10.1136/bmj.322.7300.1479 | Yes |
| 14 | Peng, R.D. | 2011 | Reproducible research in computational science. *Science*, 334, 1226-1227. DOI: 10.1126/science.1213847 | Yes |
| 15 | Pineau, J. et al. | 2021 | Improving reproducibility in machine learning research (NeurIPS 2019 program). *JMLR*, 22, 1-20. arXiv: 2003.12206 | Yes |
| 16 | REFORMS Collaboration | 2024 | Consensus-based recommendations for machine-learning-based science. *Science Advances*, 10(18). DOI: 10.1126/sciadv.adk3452 | Yes |
| 17 | Goble, C. et al. | 2020 | FAIR Computational Workflows. *Data Intelligence*, 2(1-2), 108-121. DOI: 10.1162/dint_a_00033 | Yes |
| 18 | Wilkinson, M.D. et al. | 2016 | FAIR Guiding Principles for data stewardship. *Sci Data*, 3, 160018. DOI: 10.1038/sdata.2016.18 | Yes |
| 19 | Heil, B.J. et al. | 2021 | Reproducibility standards for ML in life sciences. *Nat Methods*, 18(10), 1132-1135. DOI: 10.1038/s41592-021-01256-7 | Yes |
| 20 | ACM | 2020 | Artifact Review and Badging v1.1. ACM Publications. | Yes |
| 21 | Snilstveit, B. et al. | 2016 | Evidence & gap maps: a tool for evidence informed policy. *J Clin Epidemiol*, 79, 120-129. DOI: 10.1016/j.jclinepi.2016.05.015 | Yes |
| 22 | White, H. et al. | 2020 | Guidance for producing a Campbell evidence and gap map. *Campbell Syst Rev*, 16(4), e1125. DOI: 10.1002/cl2.1125 | Yes |
| 23 | Ioannidis, J.P.A. | 2005 | Why most published research findings are false. *PLoS Medicine*, 2(8), e124. DOI: 10.1371/journal.pmed.0020124 | Yes |
| 24 | Schneider, P. et al. | 2020 | Rethinking drug design in the artificial intelligence era. *Nat Rev Drug Discov*, 19(5), 353-364. DOI: 10.1038/s41573-019-0050-3 | Yes |
| 25 | Lipinski, C.A. et al. | 2001 | Experimental and computational approaches to estimate solubility and permeability. *Adv Drug Deliv Rev*, 46, 3-26. DOI: 10.1016/S0169-409X(00)00129-0 | Yes |
| 26 | Higgins, J.P. & Thompson, S.G. | 2003 | I-squared statistic for heterogeneity. *BMJ*, 327(7414), 557-560. DOI: 10.1136/bmj.327.7414.557 | Yes |
| 27 | Hill, A.B. | 1965 | The environment and disease: association or causation? *Proc R Soc Med*, 58, 295-300. (Bradford Hill criteria) | Yes |
| 28 | Lipinski, C.A. et al. | 1997 | Rule of Five. *Adv Drug Deliv Rev*, 23(1-3), 3-25. DOI: 10.1016/S0169-409X(96)00423-1 | Yes |
| 29 | Mitchell, M. et al. | 2019 | Model Cards for Model Reporting. *Proc FAT* 2019, 220-229. DOI: 10.1145/3287560.3287596 | Yes |

## Part A: Evidence Synthesis Methodology

### A1. Systematic Synthesis vs. Narrative Synthesis

Evidence synthesis -- the process of combining findings from multiple evidence sources into a coherent conclusion -- follows two main paradigms:

**Quantitative (meta-analytic) synthesis** pools numerical results from comparable studies using statistical methods (fixed-effect or random-effects models). Applicable when studies measure the same outcome with compatible effect measures. Produces pooled effect sizes with confidence intervals.

**Narrative synthesis** (Popay et al., 2006) combines findings textually when statistical pooling is inappropriate due to heterogeneity in methods, outcomes, or populations. Popay's framework defines four elements:
1. Developing a theory of how interventions work and why
2. Developing a preliminary synthesis (tabulation, grouping, clustering)
3. Exploring relationships within and between studies
4. Assessing robustness of the synthesis product

**SWiM guideline** (Campbell et al., 2020): When meta-analysis is not possible, SWiM provides a 9-item reporting checklist. Key items: grouping studies for synthesis, standardized metric for each group, synthesis method used, criteria for prioritizing results, investigation of heterogeneity, certainty of evidence assessment, data presentation methods.

**For Ehrlich:** Most investigations combine heterogeneous evidence (docking scores, ML predictions, bioactivity data, literature findings). Pure meta-analysis is rarely applicable. The appropriate approach is structured narrative synthesis with quantitative elements: tabulated evidence, standardized scoring, grouping by hypothesis, and explicit quality weighting. This maps to a "synthesis without meta-analysis" approach (SWiM).

### A2. GRADE Framework for Certainty of Evidence

GRADE (Grading of Recommendations Assessment, Development and Evaluation) is the internationally adopted framework for rating the certainty of a body of evidence (Guyatt et al., 2008). Over 110 organizations worldwide use GRADE, including WHO, Cochrane, and CDC.

**Four certainty levels:**

| Level | Definition | Meaning for Ehrlich |
|-------|-----------|---------------------|
| High | Very confident the true effect lies close to estimate | Multiple concordant methods, strong controls, large effect sizes |
| Moderate | Moderately confident; true effect likely close to estimate but may differ | Some concordance, adequate controls, moderate effect sizes |
| Low | Limited confidence; true effect may be substantially different | Few methods, weak controls, small or inconsistent effects |
| Very Low | Very little confidence; true effect likely substantially different | Single method, no controls, conflicting evidence |

**Five domains that downgrade certainty:**

| Domain | Computational Adaptation |
|--------|------------------------|
| Risk of bias | Model validation quality, applicability domain, training data quality |
| Inconsistency | Disagreement across methods (docking vs. ML vs. pharmacophore) |
| Indirectness | Evidence from different targets/species/assay types than the question asks about |
| Imprecision | Wide confidence intervals, small sample sizes, single predictions |
| Publication bias | Database coverage gaps, missing negative results |

**Three domains that can upgrade certainty (for observational evidence):**

| Domain | Computational Adaptation |
|--------|------------------------|
| Large effect | Very strong activity (>100-fold over baseline) |
| Dose-response | Clear SAR gradient across compound series |
| Plausible confounding would reduce effect | Conservative prediction in the presence of known biases |

**Operationalization for Ehrlich:** After evidence evaluation, the Director should assign a GRADE-like certainty level to each hypothesis outcome and to the overall investigation conclusion. The certainty assessment should explicitly name which domains caused downgrading or upgrading.

### A3. Summary of Findings Presentation

The Cochrane "Summary of Findings" (SoF) table (Higgins et al., 2019, Chapter 14) is the gold standard for presenting synthesized evidence:

**Required elements:**
- Question (PICO format or equivalent)
- For each outcome: effect estimate, certainty of evidence (GRADE), number of studies/data points, comments on interpretation
- Plain language summary of the evidence

**PRISMA 2020** (Page et al., 2021) adds 27-item reporting requirements including: synthesis methods used, certainty assessment approach, results of each synthesis, and study-level characteristics.

**For Ehrlich:** The `conclude_investigation` synthesis should produce a structured summary analogous to a SoF table, with each hypothesis outcome accompanied by its certainty level, supporting/contradicting evidence count, and a plain-language interpretation.

## Part B: Multi-Criteria Candidate Ranking

### B1. Desirability Functions

Drug candidates must simultaneously satisfy multiple criteria (potency, selectivity, ADMET, synthesizability). No single score captures overall quality.

**Harrington desirability function** (1965): Maps each property to a [0,1] desirability scale using exponential transformations. The overall desirability index (DI) is the geometric mean of individual desirabilities:

```
DI = (d1 * d2 * ... * dk)^(1/k)
```

Key property: if ANY individual desirability is 0, the overall DI is 0 -- a single unacceptable property vetoes the compound.

**Derringer-Suich extension** (1980): Three desirability function forms:
- **Larger-is-better** (e.g., potency): d = 0 below threshold, increases monotonically to d = 1 at target
- **Smaller-is-better** (e.g., toxicity): d = 1 at 0, decreases monotonically to d = 0 at threshold
- **Nominal-is-best** (e.g., LogP in range 1-3): d = 1 at target, decreasing symmetrically

**Weighted geometric mean** allows criteria to have different importance:

```
DI = (d1^w1 * d2^w2 * ... * dk^wk)^(1/sum(wi))
```

### B2. QED: Desirability Applied to Drug-Likeness

Bickerton et al. (2012) operationalized Harrington/Derringer-Suich for drug-likeness with the Quantitative Estimate of Drug-likeness (QED):

- 8 molecular properties: MW, LogP, HBA, HBD, PSA, rotatable bonds, aromatic rings, structural alerts
- Each mapped to desirability using asymmetric sigmoid functions fitted to distributions of approved oral drugs
- Overall QED = geometric mean of 8 desirabilities
- Range: 0 (non-drug-like) to ~0.95 (ideal)
- Available in RDKit: `rdkit.Chem.QED.qed(mol)`

**Key insight:** QED uses "soft" boundaries rather than Lipinski's hard cutoffs (Lipinski et al., 1997/2001). A compound with MW = 510 is not rejected outright but receives a slightly reduced desirability -- more scientifically appropriate than a binary pass/fail.

### B3. CNS MPO: Domain-Specific Desirability

Wager et al. (2016) applied the desirability concept specifically to CNS drug design:

- 6 properties: ClogP, ClogD, MW, TPSA, HBD, pKa
- Each mapped to [0,1] using linear or hump functions
- CNS MPO score = sum of 6 desirabilities (range 0-6)
- Higher is better; scores >= 4 associated with favorable ADME and BBB penetration

**Relevance:** CNS MPO demonstrates that desirability functions can be adapted to any molecular discovery domain by choosing domain-appropriate properties and fitting functions to relevant drug populations.

### B4. Pareto Optimization

When criteria are truly incommensurable (cannot be meaningfully combined into a single score), Pareto optimization identifies the non-dominated set (Nicolaou & Brown, 2013):

A compound A **dominates** compound B if A is at least as good as B on ALL criteria and strictly better on at least one. The **Pareto front** is the set of all non-dominated compounds.

**Strengths:**
- No arbitrary weighting required
- Reveals trade-offs between criteria
- Identifies compounds where improving one property necessarily worsens another

**Limitations:**
- Does not rank compounds on the Pareto front relative to each other
- As criteria increase, most compounds become non-dominated (Pareto front grows)
- Requires secondary ranking method for prioritization

### B5. TOPSIS

TOPSIS (Technique for Order Preference by Similarity to Ideal Solution; Hwang & Yoon, 1981):

1. Normalize the decision matrix (each criterion on common scale)
2. Apply weights to normalized values
3. Determine ideal best (A+) and ideal worst (A-) solutions
4. Calculate Euclidean distance of each alternative from A+ and A-
5. Rank by relative closeness: C = D- / (D+ + D-)

**For Ehrlich:** TOPSIS is useful when the Director needs to rank candidates across 4+ heterogeneous criteria (prediction score, docking score, ADMET, drug-likeness). It avoids the geometric-mean veto property of desirability functions, which may be too aggressive for early-stage computational candidates where some properties are unmeasured.

### B6. Recommended Ranking Strategy for Ehrlich

Based on the literature, a two-tier approach:

**Tier 1 -- Safety gates (hard cutoffs):**
- Predicted hERG IC50 < 10 uM -> FLAG
- PAINS substructure -> FLAG
- Predicted Ames positive -> FLAG
- Known ADMET liability (hepatotoxicity, CYP inhibition) -> FLAG
- Outside applicability domain of training model -> FLAG

**Tier 2 -- Multi-criteria ranking (soft scoring):**
- Apply desirability functions (Derringer-Suich style) to each criterion
- Compute weighted geometric mean (DI)
- Default weights: prediction_score (0.30), docking_score (0.25), drug_likeness/QED (0.20), ADMET (0.15), novelty (0.10)
- Present Pareto front for top candidates to show trade-offs

## Part C: Strength of Recommendation

### C1. GRADE Strength of Recommendation

GRADE distinguishes certainty of evidence (how sure we are) from strength of recommendation (what to do about it):

| Strength | Meaning | For Ehrlich |
|----------|---------|-------------|
| Strong | Benefits clearly outweigh risks; recommend to most | Advance candidate to experimental testing without reservations |
| Conditional | Benefits probably outweigh risks but uncertainty remains | Recommend testing with caveats; additional computational analysis advised first |
| Against | Risks probably outweigh benefits | Do not advance; evidence insufficient or contradictory |

**Factors determining strength:**
1. Certainty of evidence (GRADE level)
2. Balance of desirable vs. undesirable effects
3. Values and preferences (what matters most for this investigation)
4. Resource implications (cost of follow-up experiments)

### C2. Computational Drug Discovery Recommendation Levels

Adapted from GRADE for molecular discovery context:

| Level | Certainty | Evidence Profile | Action |
|-------|-----------|-----------------|--------|
| **Priority 1** (Strong Advance) | High/Moderate | Multiple supported hypotheses, concordant docking + ML + ADMET, controls pass, large effect sizes | Queue for synthesis and experimental testing |
| **Priority 2** (Conditional Advance) | Moderate/Low | 1-2 supported hypotheses, some method concordance, adequate controls | Additional computational validation recommended before synthesis |
| **Priority 3** (Watchlist) | Low | Partial support, limited methods, or borderline activity | Investigate further computationally; low priority for resources |
| **Priority 4** (Do Not Advance) | Very Low | Refuted hypotheses, control failures, contradictory evidence, safety flags | Archive; redirect effort |

## Part D: Limitations Assessment

### D1. Systematic Limitations Taxonomy

Ioannidis (2005) identified key factors that make research findings less likely to be true. Adapted for computational molecular discovery:

**Methodology limitations:**
- Model trained on limited chemical space (applicability domain)
- Scoring function inaccuracy (docking false positive rate)
- Feature representation limitations (fingerprint cannot capture all SAR)
- Conformational sampling insufficiency (docking explores limited pose space)

**Data limitations:**
- Database coverage gaps (organisms/targets/assay types absent from ChEMBL)
- Assay heterogeneity (mixed IC50 / Ki / EC50 / MIC in training data)
- Activity cliff proximity (similar structures, very different activities)
- Publication/reporting bias (active compounds overrepresented in literature)

**Scope limitations:**
- In silico only -- no experimental validation in this investigation
- No ADMET confirmation beyond predicted models
- Limited to chemical space accessible to search and enumeration tools
- Time-bound investigation -- not exhaustive literature coverage

**Interpretation limitations:**
- Docking scores are rank-ordering, not absolute affinity predictions
- ML probabilities require calibration (ECE check)
- Single-target focus may miss polypharmacology or off-target effects
- Resistance assessment based on known mutations only

### D2. Structured Limitations Reporting

Schneider et al. (2020) outlined "grand challenges" in AI-driven drug design. Each synthesis report should include:

1. **What was NOT tested:** Explicit list of evidence types that were planned but not obtained (e.g., "no docking performed due to lack of available crystal structure")
2. **Model validity boundaries:** Applicability domain summary (how many predictions were inside vs. outside AD)
3. **Data provenance caveats:** Source databases, their known biases, date of last update
4. **Computational approximations:** What simplifications were made (e.g., single conformer, rigid receptor, no explicit solvent)
5. **Alternative hypotheses not explored:** What other explanations or approaches were considered but not pursued

## Part E: Reproducibility Criteria

### E1. Reproducibility Spectrum

Peng (2011) defined a spectrum from full replication to publication-only:

```
Full Replication -> Reproducible Research -> Publication Only
(independent study)   (code + data + docs)   (paper only)
```

Reproducibility serves as a minimum standard when full replication is not possible.

### E2. FAIR for Computational Workflows

Goble et al. (2020) extended FAIR principles to computational workflows:

| FAIR Principle | Workflow Application | Ehrlich Implementation |
|---------------|---------------------|----------------------|
| **Findable** | Unique ID, metadata, searchable | Investigation ID (UUID), SQLite storage, event timeline |
| **Accessible** | Retrievable via standard protocols | SSE stream + REST API + event replay |
| **Interoperable** | Standard formats, vocabularies | JSON events, SMILES notation, DOI citations |
| **Reusable** | Clear provenance, licensing, documentation | Tool call logs, cost tracking, hypothesis audit trail |

### E3. ML Reproducibility Checklist

Pineau et al. (2021, NeurIPS program) and REFORMS (2024) define minimum requirements:

| Category | Requirements | Ehrlich Coverage |
|----------|-------------|-----------------|
| Data | Source, version, splits, preprocessing | Tool input logging, data source provenance |
| Model | Architecture, hyperparameters, training procedure | `train_model` records model_type, features, metrics |
| Evaluation | Metrics, confidence intervals, baselines | Negative controls, model metrics |
| Code | Available, versioned, dependencies documented | Investigation event log (partial) |
| Environment | Hardware, software versions, random seeds | Not currently tracked |

### E4. ACM Artifact Badging (v1.1, 2020)

Three independent badges:

| Badge | Requirement | Ehrlich Equivalent |
|-------|------------|-------------------|
| Artifacts Evaluated | Artifacts reviewed and functional | Event timeline complete and replayable |
| Artifacts Available | Permanently archived and accessible | SQLite DB + investigation export |
| Results Validated | Results reproduced/replicated | Full audit trail: prompt -> hypotheses -> experiments -> findings -> conclusions |

### E5. Model Card

Mitchell et al. (2019): Every ML model used in synthesis should have a minimal "model card":
- Training data source and size
- Model type and hyperparameters
- Evaluation metrics (AUROC, MCC, etc.)
- Intended use and limitations
- Applicability domain characterization

## Part F: Knowledge Gap Analysis

### F1. Evidence Gap Mapping

Snilstveit et al. (2016) and White et al. (2020) formalized evidence gap maps (EGMs):

**Framework construction:**
1. Define rows (interventions/approaches tried)
2. Define columns (outcomes/properties measured)
3. Plot evidence density in each cell
4. Empty cells = knowledge gaps
5. Low-quality cells = evidence quality gaps

**For Ehrlich:** At synthesis, the Director should construct a conceptual evidence map:
- Rows: hypotheses tested
- Columns: evidence types collected (literature, bioactivity, docking, ML, ADMET, controls)
- Each cell: present/absent/quality rating
- Gaps inform follow-up experiment recommendations

### F2. Types of Knowledge Gaps

| Gap Type | Description | Example in Ehrlich |
|----------|------------|-------------------|
| Evidence gap | No data available | No crystal structure for docking |
| Quality gap | Data exists but low quality | Only IC50 data, no Ki; single lab |
| Consistency gap | Conflicting results | Docking says good, ML says bad |
| Scope gap | Evidence exists but for different context | Data from mouse, question about human |
| Temporal gap | Evidence outdated | Resistance data from 2010, resistance landscape changed |

### F3. Follow-Up Experiment Prioritization

Gaps should be prioritized by impact and feasibility:

| Priority | Criterion |
|----------|----------|
| Critical | Gap blocks all recommendations (no activity data for top candidates) |
| High | Gap affects certainty of primary recommendation |
| Medium | Gap affects secondary recommendations or reduces confidence |
| Low | Gap is informational but doesn't change conclusions |

## Universal Components

Based on convergence across Parts A-F, 10 components represent the universally agreed minimum for a scientifically rigorous synthesis phase:

| # | Component | Description | Source(s) |
|---|-----------|-------------|-----------|
| 1 | Certainty of Evidence | Assign GRADE-adapted certainty (high/moderate/low/very low) to each hypothesis outcome and overall conclusion; explicitly state which domains caused downgrading | Guyatt et al. (2008, 2011); GRADE Working Group |
| 2 | Structured Narrative Synthesis | Tabulate all evidence grouped by hypothesis; use SWiM approach (grouping, standardized metrics, synthesis method, heterogeneity exploration) when meta-analysis is inappropriate | Popay et al. (2006); Campbell et al. (2020) |
| 3 | Multi-Criteria Candidate Ranking | Rank candidates using desirability functions (Derringer-Suich) or TOPSIS across all measured criteria; safety gates as hard cutoffs, then soft scoring for ranking | Derringer & Suich (1980); Bickerton et al. (2012); Hwang & Yoon (1981) |
| 4 | Strength of Recommendation | Classify each candidate into priority tiers (Priority 1-4) based on certainty of evidence, benefit/risk balance, and resource implications | GRADE strength framework; adapted for computational discovery |
| 5 | Limitations Disclosure | Explicitly report methodology, data, scope, and interpretation limitations using a structured taxonomy; flag what was NOT tested | Ioannidis (2005); Schneider et al. (2020) |
| 6 | Reproducibility Documentation | Record sufficient detail for workflow reproduction: data sources, model parameters, tool sequences, software versions; follow FAIR computational workflow principles | Peng (2011); Goble et al. (2020); Wilkinson et al. (2016) |
| 7 | Knowledge Gap Map | Construct hypothesis-vs-evidence-type matrix identifying gaps; classify gaps by type (evidence/quality/consistency/scope/temporal) | Snilstveit et al. (2016); White et al. (2020) |
| 8 | Follow-Up Experiment Recommendations | Prioritize knowledge gaps and recommend specific next experiments ordered by impact on confidence; distinguish computational from experimental follow-ups | Evidence gap mapping methodology |
| 9 | Summary of Findings Table | Present each hypothesis outcome with certainty level, evidence count, effect sizes, and plain-language interpretation in tabular format | Cochrane Handbook Ch. 14; PRISMA 2020 |
| 10 | Model Validation Summary | Aggregate all negative/positive control results, validation metrics, AD coverage into a single validation quality statement; flag if overall validation is insufficient | OECD (2007); Mitchell et al. (2019); Heil et al. (2021) |

## Mapping to Ehrlich's Current Implementation

| Component | Current State | Gap |
|-----------|--------------|-----|
| Certainty of Evidence | `confidence` field on hypotheses (0-1 float) | Needs GRADE-adapted certainty levels with explicit domain reasoning |
| Structured Narrative Synthesis | `summary` is free-text from Director | Needs structured grouping by hypothesis with evidence counts and quality |
| Multi-Criteria Candidate Ranking | Candidates ranked by `rank` field; scores provided but no formal ranking method | Needs desirability function or TOPSIS across all criteria |
| Strength of Recommendation | Not implemented; all candidates treated equally | Needs priority tiers in synthesis output |
| Limitations Disclosure | `limitations` list in synthesis output | Needs structured taxonomy (methodology/data/scope/interpretation) |
| Reproducibility Documentation | Event timeline and cost tracking exist | Needs explicit model card data, software versions, AD coverage summary |
| Knowledge Gap Map | Not implemented | Needs hypothesis-vs-evidence-type matrix |
| Follow-Up Experiments | Not structured; mentioned in free-text summary | Needs explicit prioritized list with gap type |
| Summary of Findings Table | InvestigationReport has 8 sections | Needs per-hypothesis tabular summary with certainty level |
| Model Validation Summary | `negative_control_summary` in synthesis | Needs aggregate validation quality statement with metrics |

## Priority for Ehrlich Implementation

**High Priority (implement in Director synthesis prompt):**
- Certainty of Evidence: GRADE-adapted levels in hypothesis_assessments output
- Strength of Recommendation: priority tier for each candidate
- Limitations Disclosure: structured limitation categories
- Follow-Up Experiments: explicit prioritized recommendations
- Summary of Findings: per-hypothesis tabular format

**Medium Priority (prompt guidance + output format):**
- Multi-Criteria Ranking: desirability scoring guidance for Director
- Knowledge Gap Map: evidence-type coverage assessment
- Model Validation Summary: aggregate validation statement

**Low Priority (infrastructure changes):**
- Full TOPSIS computation (requires code changes)
- Formal desirability function computation (requires code changes)
- Software version tracking (requires infrastructure changes)
- Evidence gap matrix visualization (requires frontend changes)
