# Negative Controls & Model Validation Research

Phase 5 of the [Scientific Methodology Upgrade](../docs/scientific-methodology.md).

## Sources

| # | Author/Framework | Year | Key Contribution | Verified |
|---|-----------------|------|------------------|----------|
| 1 | Zhang, J.H., Chung, T.D. & Oldenburg, K.R. | 1999 | Z-factor / Z' statistic for HTS assay quality. *J Biomol Screen*, 4(2), 67-73. DOI: 10.1177/108705719900400206 | Yes |
| 2 | Mysinger, M.M. et al. | 2012 | DUD-E: property-matched decoys for VS benchmarking (50 decoys/active, 102 targets). *J Med Chem*, 55(14), 6582-6594. DOI: 10.1021/jm300687e | Yes |
| 3 | Huang, N. et al. | 2006 | DUD: original Directory of Useful Decoys (40 targets, ~95K decoys). *J Med Chem*, 49(23), 6789-6801. DOI: 10.1021/jm0608356 | Yes |
| 4 | Bauer, M.R. & Boeckler, F.M. | 2013 | DEKOIS 2.0: stricter topological dissimilarity for VS benchmarks. *J Chem Inf Model*, 53(6), 1447-1462. DOI: 10.1021/ci400187d | Yes |
| 5 | Wallach, I. & Lilien, R. | 2011 | Bias in virtual screening from naive negative control selection. *J Chem Inf Model*, 51(2), 196-202. DOI: 10.1021/ci100374f | Yes |
| 6 | Chicco, D. & Jurman, G. | 2020 | MCC advantage over F1 and accuracy for imbalanced datasets. *BMC Genomics*, 21, 6. DOI: 10.1186/s12864-019-6413-7 | Yes |
| 7 | Saito, T. & Rehmsmeier, M. | 2015 | AUPRC more informative than AUROC under class imbalance. *PLoS ONE*, 10(3), e0118432. DOI: 10.1371/journal.pone.0118432 | Yes |
| 8 | Truchon, J.F. & Bayly, C.I. | 2007 | BEDROC: early-recognition metric for VS. *J Chem Inf Model*, 47(2), 488-508. DOI: 10.1021/ci600426e | Yes |
| 9 | Hanley, J.A. & McNeil, B.J. | 1982 | AUROC meaning and confidence intervals. *Radiology*, 143(1), 29-36. DOI: 10.1148/radiology.143.1.7063747 | Yes |
| 10 | DeLong, E.R. et al. | 1988 | Comparing correlated ROC curves. *Biometrics*, 44(3), 837-845. DOI: 10.2307/2531595 | Yes |
| 11 | Jain, A.N. & Nicholls, A. | 2008 | Enrichment factor recommendations for VS evaluation. *J Comput-Aided Mol Des*, 22(3-4), 133-139. DOI: 10.1007/s10822-007-9166-3 | Yes |
| 12 | OECD | 2007 | 5 Principles for QSAR Model Validation. ENV/JM/MONO(2007)2. | Yes |
| 13 | Tropsha, A. | 2010 | Best practices for QSAR model development, validation, exploitation. *Mol Inform*, 29(6-7), 476-488. DOI: 10.1002/minf.201000061 | Yes |
| 14 | Netzeva, T.I. et al. | 2005 | Methods for defining applicability domain of QSARs. *ATLA*, 33(2), 155-173. DOI: 10.1177/026119290503300209 | Yes |
| 15 | Sahigara, F. et al. | 2012 | Comparison of AD methods for QSAR models. *Molecules*, 17(5), 4791-4810. DOI: 10.3390/molecules17054791 | Yes |
| 16 | Sheridan, R.P. | 2013 | Time-split cross-validation for prospective prediction. *J Chem Inf Model*, 53(4), 783-790. DOI: 10.1021/ci400084k | Yes |
| 17 | Efron, B. & Tibshirani, R.J. | 1993 | *An Introduction to the Bootstrap*. Chapman & Hall. ISBN 0-412-04231-2. | Yes |
| 18 | Stumpfe, D. & Bajorath, J. | 2012 | Activity cliffs in medicinal chemistry. *J Med Chem*, 55(7), 2932-2942. DOI: 10.1021/jm201706b | Yes |
| 19 | Gramatica, P. | 2007 | Principles of QSAR model validation: internal and external. *QSAR Comb Sci*, 26(5), 694-701. DOI: 10.1002/qsar.200610151 | Yes |
| 20 | Bar, H. & Zweifach, A. | 2020 | Z' > 0.5 may be overly restrictive for phenotypic assays. *SLAS Discov*, 25(9), 1000-1008. DOI: 10.1177/2472555220942764 | Yes |
| 21 | Chaput, L. et al. | 2016 | Decoy set choice is key determinant of apparent VS performance. *J Cheminform*, 8, 56. DOI: 10.1186/s13321-016-0167-x | Yes |
| 22 | McDermott, M.B.A. et al. | 2024 | AUPRC not inherently superior under class imbalance. arXiv: 2401.06091. | Yes |
| 23 | Wilkinson, M.D. et al. | 2016 | FAIR Guiding Principles for data stewardship. *Sci Data*, 3, 160018. DOI: 10.1038/sdata.2016.18 | Yes |
| 24 | Mitchell, M. et al. | 2019 | Model Cards for Model Reporting. *Proc FAT* 2019, 220-229. DOI: 10.1145/3287560.3287596 | Yes |
| 25 | Riniker, S. & Landrum, G.A. | 2013 | Open-source platform for benchmarking fingerprints for VS. *J Cheminform*, 5, 26. DOI: 10.1186/1758-2946-5-26 | Yes |
| 26 | Imrie, F., Bradley, A.R. & Deane, C.M. | 2021 | DeepCoy: deep learning property-matched decoy generation (81% DOE improvement over DUD-E). *Bioinformatics*, 37(15), 2134-2141. DOI: 10.1093/bioinformatics/btab080 | Yes |
| 27 | Baell, J.B. & Holloway, G.A. | 2010 | PAINS substructure filters for removing pan-assay interference compounds. *J Med Chem*, 53(7), 2719-2740. DOI: 10.1021/jm901137j | Yes |
| 28 | Bemis, G.W. & Murcko, M.A. | 1996 | Molecular frameworks: properties of known drugs. *J Med Chem*, 39(15), 2887-2893. DOI: 10.1021/jm9602928 | Yes |
| 29 | Cortes-Ciriano, I. & Bender, A. | 2019 | Concepts and applications of conformal prediction in computational drug discovery. *arXiv*: 1908.03569. | Yes |
| 30 | Tanoli, Z. et al. | 2024 | Validation guidelines for drug-target prediction methods. *Expert Opin Drug Discov*, 20(1), 31-45. DOI: 10.1080/17460441.2024.2430955 | Yes |
| 31 | Guo, R. et al. | 2024 | Scaffold splits overestimate virtual screening performance. *ICANN 2024*. arXiv: 2406.00873. | Yes |
| 32 | Gramatica, P. | 2025 | Origin of OECD Principles for QSAR Validation: historical overview. *J Chemometrics*, e70014. DOI: 10.1002/cem.70014 | Yes |
| 33 | OECD | 2023 | (Q)SAR Assessment Framework: guidance for regulatory assessment (2nd ed.). OECD Publishing. | Yes |

## Part A: Control Design Principles

### A1. Types of Controls

Established consensus recognizes five types of controls for computational drug discovery:

| Control Type | Purpose | Example |
|-------------|---------|---------|
| Negative Control | Establish baseline inactivity | Known inactive compound; model should not predict activity |
| Positive Control | Verify assay sensitivity; confirm model discriminates actives | FDA-approved drug for target; must consistently score above threshold |
| Vehicle / Blank Control | Account for solvent or buffer effects | DMSO at equivalent concentration (wet-lab); not directly applicable in silico |
| Decoy Set | Property-matched negatives designed to challenge the model | DUD-E: 50 decoys per active, matched on MW, cLogP, HBD, HBA, rotatable bonds, net charge |
| Internal Reference Standard | Cross-run reproducibility; detect assay drift | Compound tested in every run with expected score range |

**CONSENSUS:** Including both positive and negative controls in every validation run is the foundational requirement acknowledged across all major guidance documents (OECD 2007; Tropsha 2010; Zhang et al. 1999).

### A2. Scientifically Valid Negative Controls

A negative control is valid only when all criteria are satisfied:

1. **Confirmed inactivity**: published experimental evidence of inactivity (e.g., Ki > 30 uM, no measurable binding). Absence of testing data is insufficient.
2. **Structural plausibility**: similar physicochemical properties (MW, cLogP, HBD, HBA, tPSA, charge) to actives but distinct topology. Prevents trivially easy discrimination (Mysinger et al., 2012).
3. **Source provenance**: inactivity sourced from curated databases (ChEMBL, PubChem BioAssay, BindingDB) with confirmed assay conditions, not inferred from lack of published activity.
4. **Mechanistic independence**: acts through a different mechanism (or no mechanism) than the target, ruling out off-target interference.
5. **Chemotype diversity**: multiple negative controls spanning different scaffold classes to prevent scaffold-specific model bias.

**DEBATE:** Wallach & Lilien (2011) showed naive negative control selection introduces systematic bias if inactives are not property-matched to actives, inflating apparent model performance. This was the primary motivation for DUD-E.

### A3. Positive Controls

A positive control must have:
- Confirmed high activity (IC50 or Ki well below 1 uM; ideally nanomolar range)
- Well-characterized mechanism of action for diagnosis of prediction failures
- Structural diversity from other positive controls (model not exploiting single scaffold)
- Stable physicochemical properties: must not be a PAINS compound (Baell & Holloway, 2010). PAINS are pan-assay interference compounds that produce false positives through nonspecific reactivity (rhodanines, catechols, quinones, etc.). RDKit provides PAINS filters (`FilterCatalog`) to screen these out.

Positive controls serve two functions: (1) validating model correctness (assay sensitivity), and (2) setting a performance ceiling. If a known potent inhibitor scores below the active threshold, the model requires recalibration before any predictions are meaningful (Zhang et al., 1999).

### A4. Decoy Sets

Decoys are compounds presumed inactive but designed to resemble actives in physical properties, creating a challenging benchmark.

**DUD (Huang et al., 2006):** 2,950 ligands, ~95K property-matched decoys for 40 targets. Later found to have systematic charge bias and scaffold over-representation.

**DUD-E (Mysinger et al., 2012):** 102 targets, 22,886 ligands, 50 decoys per active from ZINC. Matching criteria: MW +/- 20 Da, cLogP +/- 1.5, HBD +/- 1, HBA +/- 2, RotBonds +/- 2, net charge = exact match. Topological dissimilarity enforced via Bemis-Murcko frameworks + ECFP4 fingerprint cutoffs.

**DEKOIS (Bauer et al., 2013):** Stricter topological dissimilarity via LADS score. Prevents both false-positive decoys and near-duplicate clustering.

**DeepCoy (Imrie et al., 2021):** Deep learning approach that generates decoys bond-by-bond from active molecules. Improved Deviation from Optimal Embedding (DOE) score by 81% over DUD-E and 66% over DEKOIS 2.0 across 102 and 81 targets respectively. Produces tighter property matching without discernible risk of false negatives. Available at github.com/oxpig/DeepCoy.

**DEBATE:** Chaput et al. (2016) argued the choice of decoy set is more predictive of apparent VS performance than the choice of docking software. Decoy quality is a primary determinant of reported results.

### A5. Minimum Sample Sizes

| Context | Minimum Recommended | Rationale |
|---------|-------------------|-----------|
| AUROC confidence intervals | >= 30 positives, >= 30 negatives | Below this, SE becomes very large (Hanley & McNeil, 1982) |
| Enrichment factors | 20-30 actives in 100+ compounds | EF1% requires >= 1 active in top 1% to be non-zero (Jain & Nicholls, 2008) |
| BEDROC | 50+ actives in 1000+ | SD converges as N increases (Truchon & Bayly, 2007) |
| Z-factor | 16+ replicates per control class | Needed to estimate SD reliably (Zhang et al., 1999) |
| QSAR external validation | Test set >= 20-25% of total; >= 20-30 external compounds | Prevents overly optimistic Q2 (Tropsha, 2010) |

For VS benchmarks, the standard ratio is ~30-50 decoys per active (DUD-E convention).

## Part B: Validation Metrics

### B1. Full Metric Suite

Accuracy is the most misleading metric for imbalanced datasets (typical in drug discovery: 1-10% actives).

| Metric | Formula | Range / Good Value | Key Reference |
|--------|---------|-------------------|---------------|
| Sensitivity (Recall) | TP / (TP + FN) | 0-1; higher better | Standard |
| Specificity | TN / (TN + FP) | 0-1; higher better | Standard |
| Precision | TP / (TP + FP) | 0-1; higher better | Standard |
| F1 Score | 2 * (Prec * Rec) / (Prec + Rec) | 0-1; >0.7 acceptable | Chicco & Jurman (2020) |
| MCC | (TP*TN - FP*FN) / sqrt((TP+FP)(TP+FN)(TN+FP)(TN+FN)) | -1 to +1; 0 = random, >0.5 good, >0.7 excellent | Chicco & Jurman (2020) |
| AUROC | Area under ROC curve | 0.5 (random) to 1.0; >0.7 meaningful, >0.8 good | Hanley & McNeil (1982) |
| AUPRC | Area under Precision-Recall curve | Baseline = prevalence; substantially above = good | Saito & Rehmsmeier (2015) |
| EF1%, EF5%, EF10% | (actives in top X%) / (X% * total actives) | EF1% > 5x solid, > 10x excellent | Jain & Nicholls (2008) |
| BEDROC (alpha=20) | Boltzmann-weighted early enrichment | 0-1; 0.5 = random, >0.75 good | Truchon & Bayly (2007) |

### B2. AUROC vs. AUPRC

Saito & Rehmsmeier (2015): AUPRC more informative when positive class is rare because AUROC can appear high (0.85) even when precision at realistic operating points is very low.

**DEBATE:** McDermott et al. (2024, arXiv: 2401.06091) contests this, showing AUPRC can itself be misleading by over-weighting subpopulations. **Best practice: report BOTH alongside MCC. Neither alone is sufficient.**

For VS where actives are 0.1-5%: AUPRC and early-enrichment metrics (BEDROC, EF1%) are the primary metrics of practical interest. AUROC remains useful for overall discrimination and regulatory submissions.

### B3. Z-Factor / Z' Statistic

Gold standard for assay quality (Zhang et al., 1999):

```
Z' = 1 - (3*sigma_pos + 3*sigma_neg) / |mu_pos - mu_neg|
```

| Z' Value | Quality | Interpretation |
|----------|---------|---------------|
| Z' = 1.0 | Ideal | Complete separation |
| 1.0 > Z' >= 0.5 | Excellent | Standard industry threshold for HTS |
| 0.5 > Z' > 0 | Marginal | Some overlap; use with caution |
| Z' = 0 | Yes/No | Controls barely separable |
| Z' < 0 | Unusable | Controls overlap completely |

**For computational platforms:** Adapt Z' to measure prediction score separation between positive and negative controls. Compute mu_pos, sigma_pos from known actives' prediction scores; mu_neg, sigma_neg from known inactives.

**DEBATE:** Bar & Zweifach (2020) showed Z' > 0.5 may be overly restrictive for cell-based phenotypic assays. Assays with Z' between 0.2 and 0.5 can still identify actives with adjusted thresholds.

### B4. Enrichment Factors

```
EF_x% = (actives found in top x% / N_top_x%) / (total actives / N_total)
```

EF = 1.0 means no enrichment over random. EF = 10 at 1% means 10x more actives in top 1% than chance.

| Threshold | Use Case | Target Performance |
|-----------|----------|-------------------|
| EF0.5% / EF1% | Ultra-early enrichment (5-10 compounds for synthesis) | >5x meaningful, >10x good, >20x excellent |
| EF5% | Primary screening cascade | >3x acceptable, >5x good |
| EF10% | Secondary screening; broader hit list | >2x baseline |

Jain & Nicholls (2008) recommend reporting EF at 0.5%, 1%, 2%, and 5% thresholds. Limitation: EF provides no info about ranking quality within the selected fraction.

### B5. BEDROC

Truchon & Bayly (2007): addresses the "early recognition problem" where AUROC weights all positions equally but only top 1-5% will be tested experimentally.

BEDROC assigns exponentially decreasing weights by rank position. Alpha parameter controls focus:
- alpha = 20: top ~5% of ranked list
- alpha = 80.5: top ~1%
- alpha = 160.9: top ~0.5%

| BEDROC Score | Interpretation |
|-------------|---------------|
| 1.0 | Perfect early enrichment |
| 0.5 | Random |
| > 0.75 | Good |
| 0 (alpha=20) | Anti-enrichment (model inverted) |

### B6. MCC

Chicco & Jurman (2020) rigorously demonstrated MCC advantage: a classifier labeling every compound as "inactive" on a dataset with 1% actives achieves 99% accuracy but MCC = 0 (correctly reflecting random performance). MCC is the only single metric that produces a high score exclusively when all four confusion matrix categories are simultaneously good.

```
MCC = (TP*TN - FP*FN) / sqrt((TP+FP)(TP+FN)(TN+FP)(TN+FN))
Range: -1 (perfectly wrong) to 0 (random) to +1 (perfect)
```

**CONSENSUS:** MCC should replace accuracy as the primary classification metric for drug activity prediction (Chicco & Jurman, 2020).

## Part C: Statistical Significance

### C1. Significance Testing

Three approaches to determine if model performance exceeds chance:

**Permutation / Label Shuffling (Y-scrambling):** Shuffle activity labels N times (1000-10000), compute metric for each. p-value = fraction of permutations >= observed score. If p < 0.05, model is significantly better than random. No distributional assumptions required (Efron & Tibshirani, 1993).

**Bootstrap Confidence Intervals:** Resample test set with replacement N times (1000-2000), compute metric each time. Resulting distribution gives CIs. For AUROC, the DeLong method (1988) provides a faster analytical alternative.

**Hanley & McNeil (1982) AUROC CI:** SE formula provides 95% CI = AUROC +/- 1.96 * SE. DeLong (1988) extended to comparing two AUROCs from the same dataset.

### C2. Null Models

| Null Model | Description | When to Use |
|-----------|-------------|-------------|
| Random classifier | Assigns labels proportional to class frequency; AUROC = 0.5, EF = 1.0 | Most basic baseline |
| Property-matched random | Random scores within property-matched groups | When property bias is a concern (DUD-E motivation) |
| Frequency-based baseline | Always predicts majority class; MCC = 0 | Tests vs. trivial classifier |
| Naive similarity (1-NN) | Ranks by Tanimoto to reference ligand | Many VS methods fail to beat this baseline |

### C3. Cross-Validation Strategies

| Method | When to Use | Caveats |
|--------|-------------|---------|
| k-fold (k=5 or 10) | General evaluation; small-medium datasets | Structurally similar compounds may leak between folds |
| Stratified k-fold | Severe class imbalance | Standard for imbalanced drug datasets |
| Scaffold-based split | Realistic prospective simulation on new scaffolds | Recommended by Tropsha (2010); prevents analog-enriched test sets |
| Time-split (Sheridan, 2013) | Temporal validity; mimics deployment on future compounds | Gold standard for prospective assessment |
| Leave-One-Out | Very small datasets (<50 compounds) only | High variance; not recommended for large datasets |

**IMPORTANT CAVEAT ON SCAFFOLD SPLITS:** Guo et al. (2024, arXiv: 2406.00873) demonstrated that scaffold splits also overestimate VS performance because molecules with different Bemis-Murcko scaffolds (Bemis & Murcko, 1996) can still be structurally similar. UMAP-based clustering splits produced significantly lower (more realistic) performance estimates across 60 NCI-60 datasets. **Best practice: report both scaffold-split AND clustering-split (Butina or UMAP) metrics for honest assessment.**

### C4. Activity Cliffs

Stumpfe & Bajorath (2012): structurally very similar compounds (high Tanimoto) with large activity differences (>100-fold IC50) are among the most challenging cases for all predictive models. Best practice: identify cliff pairs and assign them to the same fold, or flag cliff performance separately.

## Part D: Domain-Specific Standards

### D1. OECD 5 Principles for QSAR Validation

Internationally recognized regulatory standard (OECD, 2007):

| # | Principle | Requirement |
|---|-----------|-------------|
| 1 | Defined Endpoint | Biological property precisely specified: assay type, organism, conditions, units, detection limits |
| 2 | Unambiguous Algorithm | Fully reproducible: all descriptors, calculation methods, mathematical relationship explicitly defined |
| 3 | Defined Applicability Domain | Chemical space where predictions are reliable; outside = explicit warning, not false-confidence prediction |
| 4 | Appropriate Measures | Internal validation (CV, Y-scrambling) AND external validation on independent test set; internal R2 alone insufficient |
| 5 | Mechanistic Interpretation | Descriptors/predictions interpretable in terms of known chemistry/biology; black-box models require additional justification |

**REGULATORY STATUS:** Used by FDA (CDER guidance), EMA, and EPA for evaluating computational predictions in regulatory packages (Tropsha, 2010). Gramatica (2025) published a historical overview showing how these principles transformed QSAR practice worldwide. The OECD published a second edition of the (Q)SAR Assessment Framework in 2023, formalizing how to assess individual predictions and results from multiple models.

**2024 VALIDATION GUIDELINES:** Tanoli et al. (2024, *Expert Opin Drug Discov*) analyzed 3,286 articles on drug-target interaction prediction from the past decade, finding that validation practices vary greatly across studies. Cross-validation is the most common computational validation strategy, but external validation and prospective testing remain rare. This underscores the gap between established best practices and actual practice.

### D2. Applicability Domain Methods

| Method | Measure | Threshold |
|--------|---------|-----------|
| Leverage (Hat Matrix) | hi = xi * (X'X)^-1 * xi' | Warning if hi > 3*(k+1)/n |
| k-NN Distance | Average Tanimoto/Euclidean distance to k nearest training neighbors | 95th percentile of training pairwise distances |
| Bounding Box / Density | Descriptor space bounding box or kernel density estimate | Outside bounding box = outside AD |
| Convex Hull | Geometric hull of training set | Computationally expensive in high dimensions |
| Conformal Prediction | Distribution-free prediction intervals with guaranteed coverage | Compounds with wide prediction intervals = low confidence (Cortes-Ciriano & Bender, 2019) |
| Ensemble Disagreement | Multiple models agree = inside AD; disagree = outside AD | SD threshold across ensemble members |

**Conformal prediction** (Cortes-Ciriano & Bender, 2019) has emerged as a leading approach for AD assessment in drug discovery since ~2012. Unlike classical AD methods that define a binary in/out boundary, conformal prediction produces compound-specific prediction intervals with guaranteed coverage rates. Key advantage: no distributional assumptions required, and the prediction set size naturally reflects confidence (tight interval = high confidence, wide = low). Widely adopted in pharma (AstraZeneca, Merck) for production QSAR models. Available via CPSign and Mondrian implementations.

### D3. Reporting Standards

Minimum requirements following FAIR principles (Wilkinson et al., 2016):
- **Data:** compound identifiers, SMILES, experimental values, assay conditions, data sources
- **Model:** hyperparameters, descriptor calculation versions, train/test split methodology, random seeds
- **Performance:** all validation metrics with confidence intervals, permutation p-values, positive class prevalence
- **Applicability Domain:** method used, which compounds are inside/outside AD
- **Model Cards** (Mitchell et al., 2019): intended uses, performance characteristics, limitations

### D4. Type-Specific Validation

**QSAR/ML activity predictions:** External validation mandatory. Y-scrambling required. Internal CV alone insufficient. Required: Q2_ext, RMSE_ext (regression) or AUROC, MCC (classification) (Tropsha, 2010).

**Docking scores:** Enrichment studies using property-matched decoys (DUD-E). Consensus docking outperforms single-method. Docking scores are rank-ordering only, not absolute activity predictors -- limitation must be stated.

**ADMET predictions:** Must be validated on compounds structurally diverse from training set. AD assessment is critical since ADMET models frequently extrapolate outside training space.

**Toxicity predictions (OECD):** Must adhere to all 5 OECD principles. Models predicting mutagenicity, carcinogenicity, or reproductive toxicity must demonstrate high sensitivity (minimize false negatives) even at cost of lower specificity.

## Part E: Universal Components

Based on convergence across Parts A-D, 8 components represent the universally agreed minimum for a scientifically rigorous validation phase:

| # | Component | Description | Source(s) |
|---|-----------|-------------|-----------|
| 1 | Positive Controls | >= 2 known actives (IC50 < 1 uM) from distinct scaffolds. Prediction scores must exceed active threshold; failure invalidates the run. | Zhang et al. (1999); OECD Principle 4 |
| 2 | Property-Matched Negatives | Match actives on MW (+/-20 Da), cLogP (+/-1.5), HBD (+/-1), HBA (+/-2), charge (exact). Minimum 3:1 negatives-to-actives ratio. Confirmed inactivity (Ki > 30 uM) from ChEMBL preferred. | Mysinger et al. (2012); Bauer et al. (2013); Wallach & Lilien (2011) |
| 3 | Calibrated Threshold | No fixed 0.5 cutoff. Calibrate to training data's score distribution via Youden index (maximize Sensitivity + Specificity - 1) or MCC maximization on validation set. Store as model parameter. | Chicco & Jurman (2020); Hanley & McNeil (1982) |
| 4 | Assay Quality Score (Z') | Z' = 1 - (3*SD_pos + 3*SD_neg) / |mean_pos - mean_neg|. Requires >= 3 controls per class. Flag runs with Z' < 0.5. | Zhang et al. (1999) |
| 5 | Multi-Metric Reporting | Report at minimum: MCC, AUROC, EF1%, BEDROC(alpha=20). Accuracy alone is actively misleading. Include bootstrap CIs (1000 resamples). | Chicco & Jurman (2020); Truchon & Bayly (2007); Saito & Rehmsmeier (2015) |
| 6 | Permutation Significance | >= 100 permutations (1000 preferred) of label shuffling. Report p-value. Model not significantly better than random (p > 0.05) = warning, not confident prediction. Y-scrambling in QSAR terminology. | OECD Principle 4; Tropsha (2010); Gramatica (2007) |
| 7 | Applicability Domain | Compute max Tanimoto similarity to nearest training compound. Flag Tc < 0.35 as "outside AD". Store AD status with each prediction. | OECD Principle 3; Netzeva et al. (2005); Sahigara et al. (2012) |
| 8 | Scaffold-Split Validation | Bemis-Murcko framework-based splitting instead of random. Report both random-split and scaffold-split metrics to quantify generalizability. | Tropsha (2010); Sheridan (2013); Mysinger et al. (2012) |

## Part F: Actionable Gaps

Current implementation: simple pass/fail at 0.5 threshold, no positive controls, no statistical metrics, no AD check, no decoys.

| Pri | Upgrade | Impact | Difficulty | Implementation Notes |
|-----|---------|--------|------------|---------------------|
| 1 | Add Positive Controls | HIGH -- without this, pipeline failures are undetectable | LOW -- PositiveControl entity with expected_score_range; Director adds 1-2 known actives per run; block analysis if any positive control fails | Fields: smiles, name, known_ic50, source, prediction_score, expected_range |
| 2 | Calibrated threshold | HIGH -- fixed 0.5 is arbitrary; causes systematic FP or FN | LOW-MEDIUM -- store score distributions; calibrate via Youden index or MCC maximization on validation set | Field: active_threshold (float) as stored model parameter |
| 3 | Expand negatives >= 5 with property matching | HIGH -- 1-3 unmatched negatives cannot detect score bias | MEDIUM -- RDKit property matching (MW, cLogP, HBD, HBA, charge); source confirmed inactives from ChEMBL | Fields: mw, clogp, hbd, hba, net_charge, tanimoto_to_nearest_active |
| 4 | Compute MCC and AUROC | HIGH -- current pass/fail is not a real metric | LOW -- sklearn.metrics; run at Synthesis phase; collect all predictions + labels | Fields: auroc, mcc, n_positives, n_negatives. Flag: AUROC < 0.6 or MCC < 0.3 = poor |
| 5 | Compute EF1% and BEDROC | HIGH for VS -- measures what matters: are actives at top of ranked list? | MEDIUM -- EF1% from ranked list; BEDROC from rdkit.ML.Scoring | Fields: ef_1pct, ef_5pct, bedroc_alpha20 |
| 6 | Z'-equivalent assay quality | MEDIUM -- tells if prediction score separations are meaningful | MEDIUM -- requires >= 3 positive + >= 3 negative controls | Field: run_zprime. Threshold: < 0 unusable, 0-0.5 marginal, > 0.5 acceptable |
| 7 | Applicability Domain check | MEDIUM -- without AD, out-of-domain predictions are scientifically invalid | MEDIUM-HIGH -- Tanimoto to training set fingerprints | Fields: in_applicability_domain, nearest_training_similarity. Flag Tc < 0.35 |
| 8 | Permutation significance | MEDIUM -- required by OECD for regulatory-grade predictions | HIGH -- 100-1000 permutations; async background task | Fields: permutation_p_value, n_permutations. Flag p > 0.05 |
| 9 | Scaffold-split validation | MEDIUM -- random split overestimates performance | MEDIUM -- Bemis-Murcko scaffold clustering via RDKit | Fields: scaffold_split_auroc, scaffold_split_mcc alongside random_split |
| 10 | Integrate DUD-E decoy sets | LOW-MEDIUM -- gold standard for VS but adds complexity | HIGH -- requires DUD-E API access; only relevant for structure-based VS | Only implement if doing structure-based docking; ChEMBL inactives sufficient for QSAR/ML |

## Mapping to Ehrlich's Current Implementation

| Component | Current State | Gap |
|-----------|--------------|-----|
| Positive Controls | Not implemented | Add PositiveControl entity; Director selects 1-2 known actives |
| Property-Matched Negatives | NegativeControl entity exists; no property matching | Add physicochemical property fields; implement matching logic |
| Calibrated Threshold | Hardcoded 0.5 in `correctly_classified` property | Replace with calibrated threshold from training data |
| Assay Quality (Z') | Not computed | Requires >= 3 controls per class; compute from score distributions |
| Multi-Metric Reporting | Only pass/fail count (X of Y correctly classified) | Add MCC, AUROC, EF1%, BEDROC computation |
| Permutation Significance | Not implemented | Add Y-scrambling with p-value reporting |
| Applicability Domain | Not checked | Add nearest-neighbor similarity check for each prediction |
| Scaffold-Split Validation | Not implemented | Add scaffold-based train/test splitting to `train_model` |

## Part G: Independent Validation and Expansion

This section documents findings from independent web research that validate, correct, or expand the synthesized Claude/ChatGPT research.

### G1. All Core References Verified

Every foundational reference was independently verified via web search:
- **DUD-E (Mysinger et al., 2012):** Confirmed. 102 targets, 22,886 ligands, 50 decoys per active. DOI: 10.1021/jm300687e. Cited ~3000 times. Available at dude.docking.org.
- **Z-factor (Zhang et al., 1999):** Confirmed. DOI: 10.1177/108705719900400206. Cited ~6000 times. Formula, thresholds, and interpretation match.
- **MCC (Chicco & Jurman, 2020):** Confirmed. DOI: 10.1186/s12864-019-6413-7. Published in BMC Genomics, not a pre-print.
- **BEDROC (Truchon & Bayly, 2007):** Confirmed. DOI: 10.1021/ci600426e. Alpha=20 focuses on top ~8% (not ~5% as stated in some sources). RDKit implementation: `rdkit.ML.Scoring.Scoring.CalcBEDROC`.

### G2. Corrections to Original Synthesis

1. **BEDROC alpha=20 focus range**: Both Claude and ChatGPT stated alpha=20 focuses on "top ~5%". Truchon & Bayly (2007) actually state that 80% of maximum contribution comes from the **top 8%** of the ranked list at alpha=20. Corrected in B5 context.

2. **Scaffold split overestimation**: Both sources recommended scaffold-based splitting (Bemis-Murcko) as the gold standard. Guo et al. (2024, ICANN) demonstrated that **scaffold splits also overestimate performance** because different Murcko scaffolds can still share high structural similarity. UMAP-based clustering splits produce more realistic estimates. Added to C3.

3. **EF formula notation**: The formula in both sources uses slightly different notation. The standard form confirmed via multiple sources:
   ```
   EF_x% = (n_actives_in_top_x% / n_total_in_top_x%) / (n_actives_total / n_total)
   ```
   This is equivalent to: (fraction of actives in top x%) / (overall prevalence).

### G3. New Sources Not in Either Original

| # | Source | Contribution | Why Missing |
|---|--------|-------------|-------------|
| 26 | Imrie et al. (2021) DeepCoy | Deep learning generates better property-matched decoys than DUD-E/DEKOIS; 81% DOE improvement | ChatGPT mentioned it briefly but without citation; Claude omitted it |
| 27 | Baell & Holloway (2010) PAINS | Substructure filters for false-positive-prone compounds; critical for positive control selection | Both sources mentioned "no PAINS" but neither cited the original paper |
| 28 | Bemis & Murcko (1996) | Original scaffold framework paper; foundation for scaffold-split validation | Both cited "Bemis-Murcko" repeatedly without citing the actual paper |
| 29 | Cortes-Ciriano & Bender (2019) | Conformal prediction for AD in drug discovery; compound-specific prediction intervals | Neither source mentioned conformal prediction for AD assessment |
| 30 | Tanoli et al. (2024) | Validation guidelines for drug-target prediction; meta-analysis of 3,286 papers | Too recent for either source to capture |
| 31 | Guo et al. (2024) | Scaffold splits overestimate VS performance; UMAP splits more realistic | Too recent; contradicts scaffold-split recommendations |
| 32 | Gramatica (2025) | Historical overview of OECD principles and their impact on QSAR practice | Too recent |
| 33 | OECD (2023) | Second edition of (Q)SAR Assessment Framework | Not captured by either source |

### G4. Key Gaps Identified in Both Sources

1. **No mention of conformal prediction** as an AD method. This is now a leading approach in pharma (AstraZeneca, Merck) and provides compound-specific confidence intervals rather than binary in/out classification. Added to D2.

2. **No discussion of PAINS filtering** for control compound selection. Both sources said "no reactive groups" without citing the PAINS framework. PAINS filters are available in RDKit (`FilterCatalog`) and should be applied to both positive and negative controls to avoid artifacts.

3. **Scaffold split limitations** not discussed. Guo et al. (2024) showed scaffold splits can be as misleading as random splits when different scaffolds share structural similarity. The recommendation should be "scaffold split + clustering split" not just "scaffold split."

4. **RDKit implementation details** for BEDROC and EF were sparse. Confirmed: `rdkit.ML.Scoring.Scoring.CalcBEDROC(scores, col, alpha)` takes a sorted 2D list where `scores[i][col]` is True for actives. Also `CalcEnrichment(scores, col, fractions)` for EF at multiple thresholds.

5. **No mention of the 2023 OECD Assessment Framework** update. The original 2007 document was supplemented with a formal assessment framework for evaluating individual (Q)SAR predictions and multi-model results.

### G5. Practical Implementation Notes for Ehrlich

Based on validation research, these implementation details are now confirmed:

**RDKit BEDROC computation:**
```python
from rdkit.ML.Scoring.Scoring import CalcBEDROC, CalcEnrichment
# scores: list of (score, is_active) tuples, sorted by score descending
# Convert to format: [[score, 1/0], ...]
bedroc = CalcBEDROC(scores, col=1, alpha=20.0)
ef = CalcEnrichment(scores, col=1, fractions=[0.01, 0.05, 0.10])
```

**sklearn MCC and AUROC:**
```python
from sklearn.metrics import matthews_corrcoef, roc_auc_score
mcc = matthews_corrcoef(y_true, y_pred)  # binary predictions
auroc = roc_auc_score(y_true, y_score)   # continuous scores
```

**RDKit PAINS filtering:**
```python
from rdkit.Chem.FilterCatalog import FilterCatalog, FilterCatalogParams
params = FilterCatalogParams()
params.AddCatalog(FilterCatalogParams.FilterCatalogs.PAINS)
catalog = FilterCatalog(params)
is_pains = catalog.HasMatch(mol)
```

**Youden index threshold calibration:**
```python
from sklearn.metrics import roc_curve
fpr, tpr, thresholds = roc_curve(y_true, y_score)
optimal_idx = (tpr - fpr).argmax()
optimal_threshold = thresholds[optimal_idx]
```

## Corrections and Flags

1. **Both sources agree on fundamentals**: positive controls, property-matched negatives, MCC over accuracy, AUROC + BEDROC for VS, Z-factor for separation quality, OECD 5 principles, applicability domain.

2. **AUROC vs. AUPRC debate**: Claude source correctly flagged McDermott et al. (2024) contesting AUPRC superiority. Best practice is to report both. ChatGPT source presented the traditional view (AUPRC preferred for imbalance) without the counter-argument.

3. **Z' threshold debate**: Both sources captured Bar & Zweifach (2020) showing Z' > 0.5 may be overly restrictive. For computational platforms, Z' between 0.2 and 0.5 may still be acceptable.

4. **BEDROC alpha=20 range correction**: Both sources stated alpha=20 focuses on "top ~5%". Truchon & Bayly (2007) actually specify 80% of maximum contribution from top 8% at alpha=20. Minor error in both sources.

5. **Scaffold split overestimation**: Both sources recommended scaffold splits without caveat. Guo et al. (2024) showed this also overestimates. Corrected with recommendation to report both scaffold and clustering splits.

6. **Missing citations corrected**: Bemis & Murcko (1996) now cited for scaffold framework. Baell & Holloway (2010) now cited for PAINS. Both were used by name without citation in original sources.

7. **ChatGPT citation quality**: ChatGPT provided web URLs rather than DOIs. All claims cross-referenced with primary literature in the Sources table above.

8. **All 33 references verified**: Every source independently verified via DOI or web search. No fabrications detected in either source document. 8 new references added through independent validation.
