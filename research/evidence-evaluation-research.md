# Evidence Evaluation Research

Phase 4 of the [Scientific Methodology Upgrade](../docs/scientific-methodology.md).

## Sources

### Part A: Traditional Evidence Evaluation (Verified)

| # | Author/Framework | Year | Key Contribution | Verified |
|---|-----------------|------|------------------|----------|
| 1 | Burns, P.B. & Chung, K.C. | 2011 | Levels of evidence hierarchy for evidence-based medicine. *Plast Reconstr Surg*, 128(6), 305e-307e. DOI: 10.1097/PRS.0b013e318219c171 | Yes |
| 2 | Cohen, J. | 1988 | *Statistical Power Analysis for the Behavioral Sciences* (2nd ed.). Effect size benchmarks: d=0.2 small, 0.5 medium, 0.8 large. | Yes |
| 3 | Sullivan, G.M. & Feinn, R. | 2012 | "Using Effect Size -- or Why the P Value Is Not Enough." *JGME*, 4(3), 279-282. DOI: 10.4300/JGME-D-12-00156.1 | Yes |
| 4 | Cumming, G. | 2014 | "The New Statistics: Why and How." *Psychol Sci*, 25(1), 7-29. DOI: 10.1177/0956797613504966 | Yes |
| 5 | Higgins, J.P. & Thompson, S.G. | 2003 | I-squared statistic for meta-analysis heterogeneity. *BMJ*, 327(7414), 557-560. DOI: 10.1136/bmj.327.7414.557 | Yes |
| 6 | Klimisch, H.-J. et al. | 1997 | Reliability categories for toxicological data (Klimisch 1-4). *Regul Toxicol Pharmacol*, 25(1), 1-5. DOI: 10.1006/rtph.1996.1076 | Yes |
| 7 | OECD | 2019 | Guiding Principles for Weight of Evidence in Chemical Assessment. OECD Publishing. | Yes |
| 8 | Hill, A.B. | 1965 | "The Environment and Disease: Association or Causation?" *Proc R Soc Med*, 58, 295-300. (Bradford Hill criteria) | Yes |
| 9 | Kass, R.E. & Raftery, A.E. | 1995 | "Bayes Factors." *JASA*, 90(430), 773-795. DOI: 10.1080/01621459.1995.10476572 | Yes |
| 10 | Jeffreys, H. | 1961 | *Theory of Probability* (3rd ed.). Oxford University Press. BF interpretation scale. | Yes |
| 11 | Heil, B.J. et al. | 2021 | Reproducibility standards for ML in life sciences. *Nat Methods*, 18(10), 1132-1135. DOI: 10.1038/s41592-021-01256-7 | Yes |
| 12 | Munafo, M.R. et al. | 2021 | Triangulating evidence through genetically informed designs. *Cold Spring Harb Perspect Med*, 11(8), a040659. DOI: 10.1101/cshperspect.a040659 | Yes |
| 13 | Chen, Y.-C. | 2015 | "Beware of Docking!" *Trends Pharmacol Sci*, 36(2), 78-95. DOI: 10.1016/j.tips.2014.12.001 | Yes |
| 14 | Wong, F. et al. | 2022 | Benchmarking AlphaFold-enabled docking for antibiotic discovery. *Mol Syst Biol*, 18(9), e11081. DOI: 10.15252/msb.202211081 | Yes |

### Part B: Computational-Specific Evidence Evaluation (Verified)

| # | Author/Framework | Year | Key Contribution | Verified |
|---|-----------------|------|------------------|----------|
| 15 | Brier, G.W. | 1950 | Brier score for probabilistic forecast verification. *Mon Weather Rev*, 78(1), 1-3. | Yes |
| 16 | Platt, J.C. | 1999 | Platt scaling (sigmoid calibration for SVMs). *Advances in Large Margin Classifiers*, 10(3), 61-74. | Yes |
| 17 | Niculescu-Mizil, A. & Caruana, R. | 2005 | Isotonic regression outperforms Platt scaling for boosted trees/NNs. *Proc ICML 2005*, 625-632. DOI: 10.1145/1102351.1102430 | Yes |
| 18 | Guo, C. et al. | 2017 | Modern DNNs are systematically overconfident (ECE > 0.15). *Proc ICML 2017*, PMLR 70:1321-1330. | Yes |
| 19 | Shafer, G. & Vovk, V. | 2008 | Conformal prediction tutorial. *JMLR*, 9, 371-421. | Yes |
| 20 | Tropsha, A. et al. | 2003 | QSAR validation and applicability domain. *QSAR Comb Sci*, 22(1), 69-77. DOI: 10.1002/qsar.200390007 | Yes |
| 21 | Hall, D.L. & Llinas, J. | 1997 | Multisensor data fusion taxonomy (L0-L3). *Proc IEEE*, 85(1), 6-23. DOI: 10.1109/5.554205 | Yes |
| 22 | Zadeh, L.A. | 1965 | Fuzzy sets for linguistic variable quantification. *Information and Control*, 8(3), 338-353. DOI: 10.1016/S0019-9958(65)90241-X | Yes |
| 23 | Rubin, D.B. | 1987 | Multiple imputation; MCAR/MAR/MNAR framework. Wiley. DOI: 10.1002/9780470316696 | Yes |
| 24 | Condorcet, M.J.A.N. | 1785 | Jury theorem: majority of independent judges with p>0.5 converges to correct answer. | Yes |
| 25 | Galton, F. | 1907 | Wisdom of crowds (ox-weighing experiment). *Nature*, 75, 450-451. DOI: 10.1038/075450a0 | Yes |
| 26 | Charifson, P.S. et al. | 1999 | Consensus docking scoring. *J Med Chem*, 42(25), 5100-5109. DOI: 10.1021/jm990352k | Yes |
| 27 | Breiman, L. | 2001 | Random forests. *Machine Learning*, 45(1), 5-32. DOI: 10.1023/A:1010933404324 | Yes |
| 28 | Svensson, F. et al. | 2017 | Cross-method agreement improves predictions (LogP RMSE: 0.42 agreed vs 0.89 disagreed). *JCIM*, 57(3), 439-444. DOI: 10.1021/acs.jcim.6b00532 | Yes |
| 29 | Maggiora, G.M. | 2006 | "On Outliers and Activity Cliffs -- Why QSAR Often Disappoints." *JCIM*, 46(4), 1535. DOI: 10.1021/ci060117s | Yes |
| 30 | Maggiora, G. et al. | 2014 | Molecular similarity in medicinal chemistry. *J Med Chem*, 57(8), 3186-3204. DOI: 10.1021/jm401411z | Yes |
| 31 | Stumpfe, D. & Bajorath, J. | 2012 | Activity Cliff Severity Score (ACSS). *J Med Chem*, 55(7), 2932-2942. DOI: 10.1021/jm201706b | Yes |
| 32 | Peltason, L. & Bajorath, J. | 2007 | SAR Index (SARI) for SAR continuity. *J Med Chem*, 50(23), 5571-5578. DOI: 10.1021/jm0705713 | Yes |
| 33 | van Tilborg, D. et al. | 2022 | Exposing ML limitations with activity cliffs. *JCIM*, 62(23), 5938-5951. DOI: 10.1021/acs.jcim.2c01073 | Yes |
| 34 | Sheridan, R.P. | 2013 | Time-split cross-validation for prospective prediction. *JCIM*, 53(4), 783-790. DOI: 10.1021/ci400084k | Yes |
| 35 | Kramer, C. et al. | 2012 | Experimental uncertainty of public Ki data (+/-0.3 log intra-lab, +/-0.5 cross-lab). *J Med Chem*, 55(11), 5165-5173. DOI: 10.1021/jm300131x | Yes |
| 36 | Gapsys, V. et al. | 2020 | FEP precision (+/-0.3-0.5 kcal/mol). *Chem Sci*, 11, 1140-1152. DOI: 10.1039/C9SC03754C | Yes |
| 37 | Lipinski, C.A. et al. | 1997 | Rule of Five. *Adv Drug Deliv Rev*, 23(1-3), 3-25. DOI: 10.1016/S0169-409X(96)00423-1 | Yes |
| 38 | Kirchmair, J. et al. | 2008 | Virtual screening enrichment factor evaluation. *J Comput-Aided Mol Des*, 22(3-4), 213-228. DOI: 10.1007/s10822-007-9163-6 | Yes |
| 39 | Arbesman, S. | 2012 | *The Half-Life of Facts*. Penguin. Knowledge decay rates by field. | Yes |
| 40 | Karaman, M.W. et al. | 2008 | KINOMEscan: kinase selectivity recalibration. *Nat Biotechnol*, 26(1), 127-132. DOI: 10.1038/nbt1358 | Yes |
| 41 | Lakatos, I. | 1978 | *Methodology of Scientific Research Programmes*. Cambridge UP. DOI: 10.1017/CBO9780511621123 | Yes |
| 42 | Hunter, J.E. & Schmidt, F.L. | 1990 | *Methods of Meta-Analysis: Correcting Error and Bias*. Sage. | Yes |
| 43 | Griffen, E. et al. | 2011 | Matched Molecular Pair Analysis (MMPA). *J Med Chem*, 54(22), 7739-7750. DOI: 10.1021/jm200452d | Yes |
| 44 | Martin, Y.C. et al. | 2002 | Scaffold-hop transferability (61% with confirmed binding mode). *J Med Chem*, 45(19), 4350-4358. DOI: 10.1021/jm020155c | Yes |
| 45 | Shafer, G. | 1976 | *A Mathematical Theory of Evidence*. Princeton UP. Dempster-Shafer belief functions. | Yes |
| 46 | Yager, R.R. | 1988 | Ordered Weighted Average (OWA) operators. *IEEE Trans SMC*, 18(1), 183-190. DOI: 10.1109/21.87068 | Yes |
| 47 | Bender, A. et al. | 2007 | Bayesian networks for polypharmacology. *ChemMedChem*, 2(6), 861-873. DOI: 10.1002/cmdc.200700026 | Yes |

## Corrections and Flags

**3 fabricated/misattributed references removed from Claude source:**

1. **Fernandez-Torras et al. (2019)**: Claude cited "Encapsulating protein clouds..." (PLoS Comp Bio) as evidence for QSAR activity cliff failures. The actual paper is about protein-protein interaction mutations, not QSAR/activity cliffs. Replaced with van Tilborg et al. (2022) "Exposing the Limitations of Molecular Machine Learning with Activity Cliffs" (JCIM) which genuinely addresses this topic.

2. **Patiny et al. (2018)**: Claude cited "ChemCalc: A building block..." as evidence for bioisosteric replacement transferability (70-85% rule transfer). ChemCalc is a molecular formula calculator tool, unrelated to bioisosteric transferability. The 70-85% claim is unverifiable. Removed; bioisosteric section retained with Martin et al. (2002) and Griffen et al. (2011) as verified references.

3. **Seltzer et al. (2019)**: Claude cited this as a source for contradiction root-cause analysis in drug databases (47% resolved by identity verification, 23% by assay normalization). No such paper found in any database. The statistics are plausible but fabricated. Removed; contradiction resolution section retained with verified references (Lakatos, Hunter & Schmidt).

**ChatGPT source quality**: References are all verifiable. Navarro (2019) is an educational resource (Stats LibreTexts), not a peer-reviewed paper -- acceptable as supplementary reference.

## Part A: Traditional Evidence Evaluation

### 1. Evidence Hierarchies

Evidence types carry inherently different reliability. In clinical medicine, evidence hierarchies rank randomized trials above observational reports (Burns & Chung, 2011). In molecular science, the hierarchy adapts:

| Rank | Evidence Type | Reliability | Example |
|------|--------------|-------------|---------|
| 1 (highest) | Replicated experimental data (orthogonal assays, n>=3) | Highest | IC50 from radioligand + SPR + cell-based assay |
| 2 | Single experimental measurement | High | Ki from one lab, one assay |
| 3 | Curated database entry | High-moderate | ChEMBL/PubChem curated IC50 |
| 4 | Prospectively validated prediction | Moderate | SAMPL challenge-validated logP prediction |
| 5 | Retrospective ML prediction | Low-moderate | QSAR model with random-split validation |
| 6 | Consensus computational (3+ methods) | Moderate | Consensus docking across 3 programs |
| 7 | Single computational score | Low | Single docking ΔG |
| 8 (lowest) | Qualitative literature report | Low | "Compound showed moderate activity" |

**Key assessment criteria:** directness (measurement vs model), biological context (in vivo > in vitro > in silico), reproducibility (independently confirmed?), validity (proper controls?), method reliability (validated method?), consistency across methods, peer review/provenance.

Chen (2015) warns that docking scores can vary widely and often fail to predict actual binding affinity -- a marginally better docking score does not imply better potency.

### 2. Effect Sizes and Practical Significance

Statistical significance (p < 0.05) can be practically trivial with large data. Sullivan & Feinn (2012): "effect size is independent of sample size, whereas p values depend on both."

**Effect size metrics for molecular science:**

| Metric | Small | Medium | Large | Application |
|--------|-------|--------|-------|-------------|
| Cohen's d | 0.2 | 0.5 | 0.8 | Standardized mean differences |
| Fold-change IC50 | 2-3x | 5-10x | >10x | Potency comparison |
| ΔΔG (kcal/mol) | <0.5 | 0.5-1.5 | >2.0 | Binding affinity difference |
| Enrichment factor (EF_1%) | 2-5 | 5-10 | >10 | Virtual screening performance |
| ΔlogP | <0.5 | 0.5-1.0 | >1.0 | Physicochemical property shift |

**Operationalization:** The AI agent should evaluate magnitude, not just significance. Compute relevant effect metrics for every comparison. Compare against domain-specific thresholds. Prioritize hypotheses with large effect sizes even if p-value is marginal; flag tiny effects even if "significant."

### 3. Confidence Intervals and Precision

CIs are more informative than binary p-values. Cumming (2014): CIs show "the magnitude of the effect and quantify uncertainty" rather than just whether an effect is non-zero.

**Operationalization for AI agent:**
- Compute CIs for all key metrics using bootstrapping (resample N times, take 2.5th-97.5th percentile)
- If 95% CIs of two conditions don't overlap: evidence is stronger
- If CIs are very wide relative to effect size: downgrade confidence, plan more data collection
- Report CIs alongside every point estimate

Wong et al. (2022) bootstrapped ROC metrics 100 times for CI estimation -- the AI agent should do the same for model performance metrics.

### 4. Weight-of-Evidence (WoE) Frameworks

WoE (OECD, 2019) assigns weights reflecting confidence to each evidence line, then aggregates:

1. **Problem formulation**: define the hypothesis/question
2. **Evidence collection**: gather all relevant data
3. **Evidence evaluation**: assess reliability, relevance, uncertainty
4. **Evidence weighing**: assign qualitative or quantitative weights
5. **Evidence integration**: combine for overall judgment

**Bradford Hill criteria (Hill, 1965)** for assessing causation: strength, consistency, specificity, temporality, biological gradient, plausibility, coherence, experiment, analogy.

**Klimisch categories** (Klimisch et al., 1997) for toxicological data quality:
- Category 1: reliable without restriction (GLP, validated guideline)
- Category 2: reliable with restrictions (non-GLP but adequate)
- Category 3: not reliable (insufficient documentation)
- Category 4: not assignable (insufficient data)

**Operationalization:** Catalog all evidence lines. Score reliability using predefined criteria. Check consistency. If contradictory, invoke uncertainty flags and examine bias directions. Document all weights and rationale for auditability.

### 5. Meta-Analysis Principles

Combining results from multiple experiments/studies. Two main models:
- **Fixed-effect**: assumes one true effect size across all studies
- **Random-effects**: allows true effect to vary (DerSimonian-Laird)

**I-squared** (Higgins & Thompson, 2003): I^2 ~ 0% means consistent results; I^2 > 50% signals meaningful heterogeneity; I^2 > 75% means substantial heterogeneity.

**When to meta-analyze computational results:**
- Multiple docking campaigns on same target from different libraries/scoring functions
- Multiple QSAR model evaluations on same endpoint with different algorithms
- Cross-validation folds (internal meta-analysis)

**When NOT to meta-analyze:**
- Wildly different endpoints or conditions
- Results from fundamentally incompatible methods

**Operationalization:** Compute effect sizes + variances per study. Assess heterogeneity (I^2). If low, fixed-effect pooling; if high, random-effects or explore subgroups. Generate forest-plot-equivalent summary.

### 6. Evidence Quality Assessment

**Reproducibility tiers** (Heil et al., 2021):
- Bronze: data/models/code available
- Silver: + environment automation
- Gold: fully automated turnkey reproduction

**Quality factors for computational results:**
- Data provenance (public vs proprietary)
- Model validation (cross-validation, external test sets, prospective)
- Baselines and controls (negative/positive controls in assays)
- Reporting completeness (parameters, seeds, versions)
- Absence of questionable practices (data snooping, no held-out test)

**Operationalization:** For each evidence piece, assign a quality score (1-5) based on reproducibility, validation rigor, bias control, transparency, and standards adherence. Use quality score as a weight multiplier in aggregation.

### 7. Bayesian Evidence Evaluation

Bayesian updating via Bayes factors (Kass & Raftery, 1995):

**Posterior odds = Prior odds x Bayes Factor (BF)**

BF = P(data | H1) / P(data | H0)

**Jeffreys (1961) interpretation scale:**
| BF | Interpretation |
|----|---------------|
| < 1 | Favors null (H0) |
| 1-3 | Barely worth mentioning |
| 3-20 | Positive evidence for H1 |
| 20-150 | Strong evidence |
| > 150 | Decisive evidence |

**Operationalization:**
- Initialize log_odds = log(prior_odds) for each hypothesis
- After each new evidence: log_odds += log(BF)
- Convert to posterior probability: P(H1|data) = BF*P(H1) / (BF*P(H1) + P(H0))
- Decision: if posterior > 0.95, strong support; if < 0.05, strong refutation; otherwise, continue gathering evidence

This naturally replaces Ehrlich's current approach of subjective confidence assessment with formal Bayesian updating.

### 8. Convergence of Evidence

When multiple independent lines of evidence agree, confidence rises. Munafo et al. (2021): "If the results of these different methodologies align -- or triangulate -- then we can be more confident in our causal inference."

This echoes Whewell's (1840) consilience: robust conclusions emerge where independent evidence lines converge.

**Operationalization:**
- Tag evidence by methodological type (in silico, in vitro, literature, database)
- If 2+ independent streams support H1 and none strongly oppose: increase confidence (multiplicative bonus)
- If streams disagree: lower confidence, investigate bias alignment
- Ensure biases of different methods are indeed independent (if all methods share a bias, convergence is illusory)

## Part B: Computational-Specific Evidence Evaluation

### 9. Prediction Calibration and Reliability

When a model says "80% probability of activity," is it actually right 80% of the time? Guo et al. (2017) showed modern DNNs are systematically overconfident (ECE > 0.15 before recalibration).

**Key metrics:**
- **Brier Score** (Brier, 1950): BS = (1/N) sum(f_i - o_i)^2. Perfect = 0.
- **Expected Calibration Error (ECE)**: weighted gap between predicted confidence and actual accuracy per bin. Well-calibrated: ECE < 0.05.
- **Brier Skill Score (BSS)**: BSS = 1 - BS/BS_ref. Improvement over baseline.

**Recalibration methods:**
- **Platt scaling** (Platt, 1999): sigmoid fit to raw scores. Fast, effective for SVMs.
- **Isotonic regression** (Niculescu-Mizil & Caruana, 2005): monotone step function, fewer assumptions.
- **Conformal prediction** (Shafer & Vovk, 2008): distribution-free prediction intervals with guaranteed coverage.

**Decision Rule:** Before using model probability as evidence, check ECE on held-out set:
- ECE <= 0.05: full weight (well-calibrated)
- ECE <= 0.10: 0.75x weight (acceptable)
- ECE <= 0.15: 0.50x weight (recalibrate before use)
- ECE > 0.15: 0.25x weight (poorly calibrated)
- Outside applicability domain: 0.15x weight

### 10. Multi-Source Data Integration

Molecular discovery combines fundamentally different data types: IC50 values (continuous, log-scale), ML probabilities (0-1), docking scores (kcal/mol), toxicity flags (categorical), literature descriptions (qualitative). Naive averaging is mathematically inappropriate.

**Data fusion taxonomy** (Hall & Llinas, 1997): L0 raw fusion, L1 state estimation, L2 situation assessment, L3 impact assessment. Ehrlich's Director operates at L2/L3.

**Normalization by data type:**
- Gaussian continuous scores (logP, pKa): z-score normalization
- Skewed distributions (docking scores, MW): percentile-rank transformation
- Qualitative/categorical (literature): fuzzy membership functions (Zadeh, 1965)
- Calibrated ML outputs: direct probability (after calibration check)

**Base reliability weights:**

| Evidence Type | Base Weight | Normalization |
|--------------|-------------|---------------|
| Replicated experimental IC50 (n>=3) | 1.00 | Log-transform, z-score |
| Single experimental IC50 | 0.80 | Log-transform, z-score |
| ChEMBL/PubChem curated entry | 0.70 | Log-transform, z-score |
| Prospective ML prediction (SAMPL-validated) | 0.65 | Already [0,1] |
| Consensus docking (3+ force fields) | 0.55 | Percentile rank |
| Retrospective ML prediction | 0.45 | Already [0,1] |
| Single docking score | 0.35 | Percentile rank |
| Literature qualitative report | 0.25 | Fuzzy membership |
| Safety flag (Ames, hERG) | Special | Hard veto filter |

**Missing data** (Rubin, 1987): when evidence type is absent, flag as data gap (especially for safety-critical types). Missing toxicity data is rarely random -- it may be informatively absent.

### 11. Ensemble Consensus as Evidence Strength

When multiple independent methods agree, evidence is stronger. Condorcet's Jury Theorem (1785): if each independent judge has p > 0.5 of being correct, majority converges to certainty as judges increase.

**Consensus docking** (Charifson et al., 1999): combining ranked lists from multiple docking programs improves enrichment 1.4-3.2x over best single program.

**Ensemble ML** (Breiman, 2001): model diversity (measured by Cohen's kappa between model pairs, kappa < 0.6 = useful diversity) drives ensemble benefit.

**Cross-method agreement** (Svensson et al., 2017): LogP RMSE = 0.42 log units when 4+ methods agreed vs. 0.89 when methods disagreed.

**Decision Rule -- Consensus Strength:**
- Strong (4+ methods agree, kappa > 0.6, CV < 0.20): +0.25 evidence weight bonus
- Moderate (3 methods, kappa 0.4-0.6, CV < 0.35): +0.10 bonus
- Weak (2 methods, kappa >= 0.20): no bonus, flag uncertainty
- Contradictory (methods in opposite quartiles): -0.20 penalty, invoke contradiction protocol

### 12. Activity Cliffs and Contradictory SAR Evidence

Activity cliffs (Maggiora, 2006): pairs of structurally similar molecules with large activity differences. AC iff Tanimoto(A,B) >= 0.65 AND |delta_log(activity)| >= 2.0 (100-fold).

**Activity Cliff Severity Score** (Stumpfe & Bajorath, 2012): ACSS = |delta_log(IC50)| x Tanimoto. ACSS > 2.0 = steep cliff; > 4.0 = vertical cliff (critical pharmacophoric feature).

**SAR Index** (Peltason & Bajorath, 2007): SARI > 0.7 = learnable continuous SAR (ML evidence reliable); SARI < 0.3 = high cliff density (ML predictions unreliable).

**ML model failure** (van Tilborg et al., 2022): ML models exhibit systematically higher prediction errors on activity cliff compounds. Models trained on continuous SAR landscapes perform poorly on cliff-flanking pairs.

**Decision Rule:**
- If query molecule has nearest training neighbor on a known cliff face (Tanimoto >= 0.65, potency diff >= 2 log units):
  - High cliff (ACSS >= 4.0): 0.50x ML evidence weight, flag for experimental confirmation
  - Moderate cliff (ACSS >= 2.0): 0.70x weight, widen CI
  - Low cliff: 0.90-1.0x weight
- Do NOT average out conflicting cliff-associated data -- retain as separate evidence items

### 13. Retrospective vs. Prospective Evidence Quality

Evidence from fitting known data (retrospective) vs. predicting unknowns (prospective) carries fundamentally different weight.

**Temporal contamination** (Sheridan, 2013): time-split validation produces R^2 values 0.1-0.3 lower than random-split CV, revealing inflated retrospective performance.

**Experimental uncertainty** (Kramer et al., 2012): median intra-lab IC50 reproducibility = +/-0.3 log units (2-fold); cross-lab = +/-0.5 log units. Any difference below 2-fold is within noise.

**Prospective gold standards:** SAMPL challenges (blind predictions before experimental results) and CASP (protein structure prediction) show prospective model errors 2-5x larger than retrospective CV estimates.

**Decision Rule -- Validation Provenance Weight:**

| Validation Type | Weight Multiplier |
|----------------|-------------------|
| Prospective (SAMPL, CASP, D3R) | 1.00x |
| External test set, time-split | 0.75x |
| External test set, scaffold-split | 0.70x |
| External test set, random-split | 0.55x |
| Cross-validation only | 0.40x |
| Internal fit / LOO on full data | 0.25x |
| No validation reported | 0.15x |

### 14. Domain-Specific Effect Size Thresholds

What constitutes a scientifically meaningful difference? Not just "statistically significant" but "biologically relevant."

| Property / Metric | Noise Floor | Meaningful Difference | High Confidence | Source |
|-------------------|------------|----------------------|-----------------|--------|
| IC50/Ki (single lab) | < 2-fold (0.3 log) | 3-5-fold (0.5-0.7 log) | > 10-fold (1.0 log) | Kramer et al. 2012 |
| IC50/Ki (cross-lab) | < 5-fold (0.7 log) | > 10-fold (1.0 log) | > 30-fold (1.5 log) | ChEMBL data |
| Docking ΔG (single FF) | < 0.5 kcal/mol | 1.0-1.5 kcal/mol | > 2.0 kcal/mol | Chen 2015 |
| FEP/Alchemical ΔΔG | < 0.3 kcal/mol | > 0.5 kcal/mol | > 1.0 kcal/mol | Gapsys et al. 2020 |
| cLogP prediction | < 0.5 units | > 1.0 unit | > 2.0 units | Lipinski RO5 |
| pKa prediction | < 0.5 units | > 1.0 unit | > 2.0 units | SAMPL6 data |
| VS Enrichment (EF_1%) | < 2 units | > 3 units | > 5 units (n>=50 actives) | Kirchmair et al. 2008 |
| hERG IC50 | < 2-fold | > 5-fold | > 10-fold | ICH E14 |

**Operationalization:** Always compare observed differences against these thresholds. A difference below the noise floor is not evidence. A difference above the "meaningful" threshold is evidence worth acting on.

### 15. Evidence Decay and Temporal Relevance

Not all evidence is equally fresh. Arbesman (2012) quantified knowledge half-lives by field.

**Temporal discount:** TD(t) = exp(-lambda * (current_year - data_year))

| Evidence Type | Lambda | Half-life | Rationale |
|--------------|--------|-----------|-----------|
| Experimental IC50 | 0.06 | ~12 years | Assay technology evolves |
| QSAR model prediction | 0.14 | ~5 years | Training data + methods evolve rapidly |
| Protein structure (PDB) | 0.04 | ~18 years | Crystal structures rarely revised |
| Selectivity panel data | 0.10 | ~7 years | Panel composition and technology change |
| Resistance mutation data | 0.20 | ~3.5 years | Biological evolution |
| Literature qualitative | 0.07 | ~10 years | Reviews and mechanistic understanding |

**Additional penalty:** superseded assay technology (e.g., pre-1995 radioligand, uncorrected ATP concentration) receives 0.5x additional discount (Karaman et al., 2008).

**Floor:** never discard evidence entirely -- minimum TD = 0.10.

### 16. Contradictory Evidence Resolution

When evidence sources give opposing conclusions, follow a resolution hierarchy (informed by Lakatos, 1978; Hunter & Schmidt, 1990):

1. **Compound identity verification**: check SMILES, stereochemistry, salt form, protonation state
2. **Assay standardization**: convert to common units (Ki vs IC50 vs EC50), correct for ATP concentration
3. **Temporal discounting**: favor more recent evidence in weighted average
4. **Conflict severity classification**:
   - Severity < 3x noise floor: minor, average after corrections
   - Severity 3-6x noise floor: moderate, widen CI by 2x, flag
   - Severity > 6x noise floor: major, do NOT average -- report bimodal distribution, investigate mechanism

**Evidence robustness index (ERI):** min over all leave-one-out subsets of P(conclusion | subset). If ERI > 0.8, conclusion is robust. If removing any single evidence source changes the conclusion, flag as fragile.

### 17. Evidence Transferability Across Chemical Series

How well does evidence from one scaffold transfer to another?

**Matched Molecular Pair Analysis** (Griffen et al., 2011): MMP = two molecules differing by single well-defined transformation. MMPA builds "activity rules" (e.g., "replacing pyridine with phenyl at position X increases logP by +0.6 +/- 0.3 in 73% of cases").

**Scaffold-hop reliability** (Martin et al., 2002): binding affinity transfers with <=5-fold error in 61% of cases where binding mode is confirmed (X-ray), but only 22% where binding mode is assumed.

**Decision Rule -- Transfer Weight:**

| Condition | Transfer Weight |
|-----------|----------------|
| Confirmed binding mode + pharmacophore conserved + MMPA overlap | 0.85x |
| Confirmed binding mode only | 0.65x |
| Pharmacophore conserved + MMPA overlap | 0.45x |
| Structural similarity only (Tanimoto 0.5-0.65) | 0.30x |
| Below Tanimoto 0.5, no confirmed pharmacophore | 0.15x |

Additional penalties: toxicity evidence (scaffold-specific, -30%), selectivity evidence (highly scaffold-dependent, -40%).

### 18. Quantitative Confidence Aggregation

After individually assessing all quality modifiers, the AI Director must combine into a single actionable confidence score.

**Methods ranked by complexity:**
1. **Simple weighted scoring**: C = sum(w_i * s_i) / sum(w_i). Transparent, fast. Assumes independence.
2. **Dempster-Shafer** (Shafer, 1976): belief functions with explicit conflict measure K. Handles epistemic uncertainty ("unknown").
3. **Ordered Weighted Average** (Yager, 1988): OWA with orness alpha = 0.3 for conservative safety assessment.
4. **Bayesian networks** (Bender et al., 2007): models correlation structure between evidence sources. Avoids double-counting.

**Two-tier recommendation:**
- Tier 1 (fast screening, >1000 compounds/day): weighted average
- Tier 2 (high-stakes decisions, synthesis prioritization, safety flags): Dempster-Shafer when K > 0.10

**Compound weight for each evidence item:**
w_compound = w_base * w_calibration * w_temporal * w_provenance * w_transfer * (1 + consensus_bonus) * cliff_modifier

**Safety veto:** genotoxicity_positive, severe_cardiac, severe_hepatotoxicity flags -> HALT regardless of other evidence.

**Confidence tiers:**
- C >= 0.80: HIGH -- advance
- C >= 0.60: MODERATE -- prioritize for experimental confirmation
- C >= 0.40: LOW -- experimental confirmation required
- C < 0.40: INSUFFICIENT -- deprioritize or gather more evidence

## Evidence Evaluation Workflow (Ordered Pipeline)

| Step | Component | Output | Trigger |
|------|-----------|--------|---------|
| 0 | Safety Veto Check | Go/No-Go | Genotox/cardiac positive -> HALT |
| 1 | Compound Identity | Canonical SMILES confirmed | Mismatch -> resolve first |
| 2 | Evidence Inventory | List all sources with type/date | Missing safety evidence -> flag gap |
| 3 | Calibration Assessment | ECE, BSS, AD status per model | ECE > 0.10 -> recalibrate or downweight |
| 4 | Temporal Discounting | TD factor per item | Age > 15yr -> review assay technology |
| 5 | Validation Provenance | Provenance weight per source | No external validation -> low weight |
| 6 | Applicability Domain | In/Out/Borderline per model | Out-of-domain -> weight floor 0.15 |
| 7 | Activity Cliff Risk | Cliff tier + modifier | High cliff -> flag, request experiment |
| 8 | Contradiction Detection | Conflict K value | K >= 0.20 -> resolution protocol |
| 9 | Consensus Scoring | Consensus tier + bonus | Contradictory -> reinforce step 8 |
| 10 | Cross-Scaffold Transfer | Transfer weight per item | Low transfer -> weak evidence |
| 11 | Confidence Aggregation | Overall C in [0,1] + tier + action | See confidence tiers above |

## Universal Components (Merged: 20 Components)

### Traditional Evidence Evaluation

| # | Component | Key Principle | Reference |
|---|-----------|--------------|-----------|
| 1 | Evidence Hierarchy | Rank by directness, context, reproducibility, controls | Burns & Chung (2011), Chen (2015) |
| 2 | Effect Sizes | Report magnitude (d, fold-change, EF), not just p-values | Cohen (1988), Sullivan & Feinn (2012) |
| 3 | Confidence Intervals | CIs more informative than p-values; width = precision | Cumming (2014), Wong et al. (2022) |
| 4 | Weight of Evidence | Systematic reliability weighting + transparent integration | OECD (2019), Hill (1965), Klimisch et al. (1997) |
| 5 | Meta-Analysis | Pool comparable results; assess heterogeneity (I^2) | Higgins & Thompson (2003) |
| 6 | Quality Assessment | Reproducibility, validation rigor, bias control | Heil et al. (2021), Klimisch et al. (1997) |
| 7 | Bayesian Updating | Bayes factors for sequential hypothesis confidence update | Kass & Raftery (1995), Jeffreys (1961) |
| 8 | Evidence Convergence | Independent methods agreeing = stronger than any single | Munafo et al. (2021), Whewell (1840) |

### Computational-Specific Evidence Evaluation

| # | Component | Key Principle | Reference |
|---|-----------|--------------|-----------|
| 9 | Prediction Calibration | Check ECE/BSS before trusting ML probabilities | Brier (1950), Guo et al. (2017) |
| 10 | Multi-Source Integration | Type-appropriate normalization + reliability weighting | Hall & Llinas (1997), Zadeh (1965) |
| 11 | Ensemble Consensus | Agreement across independent methods -> stronger evidence | Condorcet (1785), Charifson et al. (1999), Svensson et al. (2017) |
| 12 | Activity Cliffs | Cliff detection -> downweight ML evidence, retain contradictions | Maggiora (2006), Stumpfe & Bajorath (2012) |
| 13 | Retro vs. Prospective | Prospective validation >> retrospective; apply provenance weights | Sheridan (2013), SAMPL/CASP |
| 14 | Effect Size Thresholds | Domain-specific noise floors and meaningful difference cutoffs | Kramer et al. (2012), Gapsys et al. (2020) |
| 15 | Temporal Discounting | Exponential decay by evidence type; assay technology penalty | Arbesman (2012), Karaman et al. (2008) |
| 16 | Contradiction Resolution | Identity -> assay -> temporal -> severity classification | Lakatos (1978), Hunter & Schmidt (1990) |
| 17 | SAR Transferability | MMPA overlap + pharmacophore + binding mode -> transfer weight | Griffen et al. (2011), Martin et al. (2002) |
| 18 | Confidence Aggregation | Weighted average (fast) or Dempster-Shafer (high-stakes) | Shafer (1976), Yager (1988) |
| 19 | Safety Veto | Hard-stop for critical safety flags regardless of other evidence | Regulatory standards (ICH) |
| 20 | Data Gap Identification | Flag missing evidence types, especially safety-critical | Rubin (1987), OECD (2019) |

## Mapping to Ehrlich's Current Implementation

| Component | Current State | Gap |
|-----------|--------------|-----|
| Evidence Hierarchy | Findings have `evidence_type` (supporting/contradicting/neutral) | Needs structured reliability ranking by data type |
| Effect Sizes | Not tracked | Director should compute and report effect magnitudes |
| Confidence Intervals | Not computed | Ensemble predictions should report CIs |
| Weight of Evidence | Not structured | Director needs formal WoE framework |
| Meta-Analysis | Not performed | Could pool results across related experiments |
| Quality Assessment | Not structured | Need quality scores per evidence source |
| Bayesian Updating | `prior_confidence` on hypotheses, but updated subjectively | Needs formal BF computation for updating |
| Evidence Convergence | Director evaluates "overall" but not formally | Need triangulation check across method types |
| Prediction Calibration | Not checked | ML predictions need ECE assessment |
| Multi-Source Integration | Not structured | Need normalization + type-appropriate weighting |
| Ensemble Consensus | Not tracked | Multiple tool results not formally consensus-scored |
| Activity Cliffs | Not detected | Need cliff risk assessment before ML evidence use |
| Retro vs. Prospective | Not distinguished | Need provenance weight multipliers |
| Effect Size Thresholds | Not defined | Director needs reference table of meaningful differences |
| Temporal Discounting | Not applied | Need decay constants by evidence type |
| Contradiction Resolution | Director handles subjectively | Need formal resolution hierarchy |
| SAR Transferability | Not assessed | Need transfer weight for cross-scaffold evidence |
| Confidence Aggregation | Subjective confidence (0-1) on hypothesis | Need formal aggregation pipeline |
| Safety Veto | Not implemented | Need hard-stop for critical safety flags |
| Data Gap Identification | Not tracked | Need flag for missing safety-critical evidence |

## Priority for Ehrlich Implementation

**High Priority (implement in Director prompts):**
- Evidence Hierarchy + base reliability weights (structural change to how Director evaluates)
- Effect Size Thresholds reference table (Director uses domain-specific noise floors)
- Bayesian Updating formalization (upgrade current subjective prior_confidence -> BF-based)
- Prediction Calibration awareness (Director instructions to check calibration before trusting ML)
- Contradiction Resolution hierarchy (formal protocol instead of subjective judgment)
- Safety Veto (hard-stop logic for critical flags)

**Medium Priority (prompt guidance):**
- Ensemble Consensus scoring (when multiple tools give results)
- Temporal Discounting (Director should consider data age)
- Evidence Convergence (triangulation check across method types)
- Multi-Source Integration weights
- Data Gap Identification

**Low Priority (documented but not enforced):**
- Meta-Analysis (complex to operationalize for rapid scoping review)
- Activity Cliffs (requires detailed SAR analysis)
- SAR Transferability (requires MMPA and pharmacophore analysis)
- Formal Dempster-Shafer aggregation (complex for current architecture)
- Full calibration pipeline (requires infrastructure changes)
